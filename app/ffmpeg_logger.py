# ffmpeg_logger.py
import logging
from pathlib import Path
import os
import datetime
import time
import re
import gc
from config import logger, handle_ffmpeg_error


class FFmpegLogger:
    """
    # On centralise la gestion des logs FFmpeg avec rotation automatique
    """

    def __init__(self, channel_name: str):
        self.channel_name = channel_name
        self.base_dir = Path("/app/logs/ffmpeg")
        self.base_dir.mkdir(parents=True, exist_ok=True)

        # Fichiers de logs
        self.main_log = self.base_dir / f"{channel_name}_ffmpeg.log"
        self.progress_log = self.base_dir / f"{channel_name}_progress.log"

        # Taille max des logs (5MB)
        self.max_log_size = 5 * 1024 * 1024

        # On v√©rifie/nettoie les logs existants
        self._init_logs()

    def _init_logs(self):
        """Initialise les logs et v√©rifie leur taille"""
        for log_file in [self.main_log, self.progress_log]:
            # Cr√©e le fichier s'il n'existe pas
            if not log_file.exists():
                log_file.touch()
            else:
                # V√©rifie la taille et applique rotation si n√©cessaire
                self._check_and_rotate_log(log_file)

    # M√âTHODE SUPPRIM√âE: _track_segment_size()
    # Cette m√©thode √©tait redondante avec le tracking fait par StatsCollector
    # via l'analyse des logs Nginx. De plus, elle utilisait gc.get_objects()
    # ce qui est extr√™mement inefficace.

    def log_segment(self, segment_path: str, size: int):
        """
        Log des infos sur les segments g√©n√©r√©s directement dans le log principal.

        Note: Le tracking des stats est maintenant g√©r√© par StatsCollector via
        l'analyse des logs Nginx, ce qui √©vite la duplication et l'utilisation
        co√ªteuse de gc.get_objects().
        """
        segment_info = (
            f"{datetime.datetime.now()} - Segment {segment_path}: {size} bytes"
        )

        # On utilise le logger principal plut√¥t qu'un fichier s√©par√©
        logger.debug(f"[{self.channel_name}] {segment_info}")

        # SUPPRIM√â: Le tracking via gc.get_objects() est inefficace et redondant
        # Les stats de segments sont maintenant calcul√©es par StatsCollector
        # en analysant les logs Nginx, ce qui est plus fiable et performant

    def _check_and_rotate_log(self, log_file: Path):
        """V√©rifie la taille d'un fichier log et fait une rotation si n√©cessaire"""
        try:
            if not log_file.exists():
                log_file.touch()
                logger.info(f"‚úÖ Created log file: {log_file.name}")
            elif log_file.stat().st_size > self.max_log_size:
                # Format du timestamp
                timestamp = time.strftime("%Y%m%d_%H%M%S")

                # Nouveau nom avec timestamp
                backup_name = f"{log_file.stem}_{timestamp}{log_file.suffix}"
                backup_path = log_file.parent / backup_name

                # Copie le contenu actuel vers le backup
                import shutil
                shutil.copy2(log_file, backup_path)
                
                # Vide le fichier de log actuel
                log_file.write_text("")

                logger.info(
                    f"üîÑ Rotation du log {log_file.name} -> {backup_name} (taille > {self.max_log_size/1024/1024:.1f}MB)"
                )

                # Limite le nombre d'archives (garde les 5 plus r√©centes)
                self._cleanup_old_logs(log_file.stem, log_file.suffix)

        except Exception as e:
            logger.error(f"‚ùå Erreur rotation log {log_file}: {e}")

    def _cleanup_old_logs(self, base_name: str, suffix: str):
        """Garde seulement les 5 fichiers de log les plus r√©cents pour un type donn√©"""
        try:
            # Liste tous les fichiers de log archiv√©s pour ce type
            pattern = f"{base_name}_*{suffix}"
            archived_logs = list(self.base_dir.glob(pattern))

            # Trie par date de modification (du plus r√©cent au plus ancien)
            archived_logs.sort(key=lambda p: p.stat().st_mtime, reverse=True)

            # Supprime les logs les plus anciens (au-del√† des 5 premiers)
            if len(archived_logs) > 5:
                for old_log in archived_logs[5:]:
                    try:
                        old_log.unlink()
                        logger.info(f"üóëÔ∏è Suppression de l'ancien log: {old_log.name}")
                    except Exception as e:
                        logger.error(f"‚ùå Erreur suppression {old_log.name}: {e}")

        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage des anciens logs: {e}")

    def get_progress_file(self) -> Path:
        """Renvoie le chemin du fichier de progression"""
        # V√©rifie/effectue une rotation si n√©cessaire
        self._check_and_rotate_log(self.progress_log)
        return self.progress_log

    def get_main_log_file(self) -> Path:
        """Renvoie le chemin du log principal apr√®s v√©rification de taille"""
        # V√©rifie/effectue une rotation si n√©cessaire
        self._check_and_rotate_log(self.main_log)
        return self.main_log
        
    def process_error_logs(self):
        """
        Traite les logs FFmpeg pour d√©tecter et g√©rer les erreurs
        """
        try:
            # V√©rifier que le fichier de log existe
            if not self.main_log.exists():
                return
                
            # Lire les 20 derni√®res lignes du log pour rechercher des erreurs r√©centes
            with open(self.main_log, 'r', encoding='utf-8', errors='ignore') as f:
                # Lire tout le fichier
                lines = f.readlines()
                # Prendre les 20 derni√®res lignes
                last_lines = lines[-20:] if len(lines) >= 20 else lines
                
            # Rechercher des patterns d'erreur dans les derni√®res lignes
            error_patterns = [
                r"Invalid data found when processing input",
                r"Could not find file",
                r"Error while decoding stream",
                r"corrupt.*?frame",
                r"Fichier d'entr√©e introuvable"
            ]

            # Patterns √† ignorer (erreurs b√©nignes normales dans HLS)
            ignore_patterns = [
                r"failed to delete old segment.*No such file or directory",  # Normal: cleaner a d√©j√† supprim√©
                r"hls muxer.*failed to delete",  # M√™me chose, format variant
            ]

            for line in last_lines:
                # Ignorer les erreurs b√©nignes
                should_ignore = False
                for ignore_pattern in ignore_patterns:
                    if re.search(ignore_pattern, line, re.IGNORECASE):
                        should_ignore = True
                        break

                if should_ignore:
                    continue

                # V√©rifier les vraies erreurs
                for pattern in error_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        logger.warning(f"[{self.channel_name}] üîç Erreur FFmpeg d√©tect√©e: {line.strip()}")
                        # Appeler le gestionnaire d'erreurs
                        handle_ffmpeg_error(self.channel_name, line)
                        return  # Une seule erreur √† la fois suffit
                        
        except Exception as e:
            logger.error(f"[{self.channel_name}] ‚ùå Erreur traitement logs FFmpeg: {e}")
            
    def check_for_errors(self):
        """
        V√©rifie les logs pour des erreurs et les traite si n√©cessaire.
        √Ä appeler p√©riodiquement.
        """
        # V√©rifier la taille des logs avant traitement
        self._check_and_rotate_log(self.main_log)
        # Traiter les logs pour d√©tecter les erreurs
        self.process_error_logs()
