# ffmpeg_logger.py
import logging
from pathlib import Path
import os
import datetime
import time
import re
import gc
from config import logger, handle_ffmpeg_error


class FFmpegLogger:
    """
    # On centralise la gestion des logs FFmpeg avec rotation automatique
    """

    def __init__(self, channel_name: str):
        self.channel_name = channel_name
        self.base_dir = Path("/app/logs/ffmpeg")
        self.base_dir.mkdir(parents=True, exist_ok=True)

        # Fichiers de logs
        self.main_log = self.base_dir / f"{channel_name}_ffmpeg.log"
        self.progress_log = self.base_dir / f"{channel_name}_progress.log"

        # Taille max des logs (5MB)
        self.max_log_size = 5 * 1024 * 1024

        # On v√©rifie/nettoie les logs existants
        self._init_logs()

    def _init_logs(self):
        """Initialise les logs et v√©rifie leur taille"""
        for log_file in [self.main_log, self.progress_log]:
            # Cr√©e le fichier s'il n'existe pas
            if not log_file.exists():
                log_file.touch()
            else:
                # V√©rifie la taille et applique rotation si n√©cessaire
                self._check_and_rotate_log(log_file)

    # Ajouter cette m√©thode √† la classe FFmpegLogger dans ffmpeg_logger.py

    def _track_segment_size(self, segment_path: str, stats_collector=None):
        """Suit la taille des segments pour les statistiques"""
        try:
            # V√©rifier que le fichier existe
            if not os.path.exists(segment_path):
                return

            # Obtenir la taille du segment
            segment_size = os.path.getsize(segment_path)

            # Loguer la taille dans le fichier de log
            self.log_segment(segment_path, segment_size)

            # Si on a un StatsCollector, mettre √† jour les stats
            if stats_collector:
                # Extraire le nom de la cha√Æne et l'ID du segment
                segment_path_obj = Path(segment_path)
                channel_name = segment_path_obj.parent.name
                segment_id_match = re.search(
                    r"segment_(\d+)\.ts", segment_path_obj.name
                )

                if segment_id_match and channel_name:
                    segment_id = segment_id_match.group(1)
                    stats_collector.update_segment_stats(
                        channel_name, segment_id, segment_size
                    )
        except Exception as e:
            logger.error(f"‚ùå Erreur tracking segment {segment_path}: {e}")

    def log_segment(self, segment_path: str, size: int):
        """Log des infos sur les segments g√©n√©r√©s directement dans le log principal"""
        segment_info = (
            f"{datetime.datetime.now()} - Segment {segment_path}: {size} bytes"
        )

        # On utilise le logger principal plut√¥t qu'un fichier s√©par√©
        logger.debug(f"[{self.channel_name}] {segment_info}")

        # Si on a acc√®s au StatsCollector, mettre √† jour les stats
        from iptv_manager import IPTVManager

        for manager_instance in [
            obj for obj in gc.get_objects() if isinstance(obj, IPTVManager)
        ]:
            if hasattr(manager_instance, "stats_collector"):
                self._track_segment_size(segment_path, manager_instance.stats_collector)

    def _check_and_rotate_log(self, log_file: Path):
        """V√©rifie la taille d'un fichier log et fait une rotation si n√©cessaire"""
        try:
            if not log_file.exists():
                log_file.touch()
                logger.info(f"‚úÖ Created log file: {log_file.name}")
            elif log_file.stat().st_size > self.max_log_size:
                # Format du timestamp
                timestamp = time.strftime("%Y%m%d_%H%M%S")

                # Nouveau nom avec timestamp
                backup_name = f"{log_file.stem}_{timestamp}{log_file.suffix}"
                backup_path = log_file.parent / backup_name

                # Copie le contenu actuel vers le backup
                import shutil
                shutil.copy2(log_file, backup_path)
                
                # Vide le fichier de log actuel
                log_file.write_text("")

                logger.info(
                    f"üîÑ Rotation du log {log_file.name} -> {backup_name} (taille > {self.max_log_size/1024/1024:.1f}MB)"
                )

                # Limite le nombre d'archives (garde les 5 plus r√©centes)
                self._cleanup_old_logs(log_file.stem, log_file.suffix)

        except Exception as e:
            logger.error(f"‚ùå Erreur rotation log {log_file}: {e}")

    def _cleanup_old_logs(self, base_name: str, suffix: str):
        """Garde seulement les 5 fichiers de log les plus r√©cents pour un type donn√©"""
        try:
            # Liste tous les fichiers de log archiv√©s pour ce type
            pattern = f"{base_name}_*{suffix}"
            archived_logs = list(self.base_dir.glob(pattern))

            # Trie par date de modification (du plus r√©cent au plus ancien)
            archived_logs.sort(key=lambda p: p.stat().st_mtime, reverse=True)

            # Supprime les logs les plus anciens (au-del√† des 5 premiers)
            if len(archived_logs) > 5:
                for old_log in archived_logs[5:]:
                    try:
                        old_log.unlink()
                        logger.info(f"üóëÔ∏è Suppression de l'ancien log: {old_log.name}")
                    except Exception as e:
                        logger.error(f"‚ùå Erreur suppression {old_log.name}: {e}")

        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage des anciens logs: {e}")

    def get_progress_file(self) -> Path:
        """Renvoie le chemin du fichier de progression"""
        # V√©rifie/effectue une rotation si n√©cessaire
        self._check_and_rotate_log(self.progress_log)
        return self.progress_log

    def get_main_log_file(self) -> Path:
        """Renvoie le chemin du log principal apr√®s v√©rification de taille"""
        # V√©rifie/effectue une rotation si n√©cessaire
        self._check_and_rotate_log(self.main_log)
        return self.main_log
        
    def process_error_logs(self):
        """
        Traite les logs FFmpeg pour d√©tecter et g√©rer les erreurs
        """
        try:
            # V√©rifier que le fichier de log existe
            if not self.main_log.exists():
                return
                
            # Lire les 20 derni√®res lignes du log pour rechercher des erreurs r√©centes
            with open(self.main_log, 'r', encoding='utf-8', errors='ignore') as f:
                # Lire tout le fichier
                lines = f.readlines()
                # Prendre les 20 derni√®res lignes
                last_lines = lines[-20:] if len(lines) >= 20 else lines
                
            # Rechercher des patterns d'erreur dans les derni√®res lignes
            error_patterns = [
                r"No such file or directory",
                r"Invalid data found when processing input",
                r"Could not find file",
                r"Error while decoding stream",
                r"corrupt.*?frame",
                r"Fichier d'entr√©e introuvable"
            ]
            
            for line in last_lines:
                for pattern in error_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        logger.warning(f"[{self.channel_name}] üîç Erreur FFmpeg d√©tect√©e: {line.strip()}")
                        # Appeler le gestionnaire d'erreurs
                        handle_ffmpeg_error(self.channel_name, line)
                        return  # Une seule erreur √† la fois suffit
                        
        except Exception as e:
            logger.error(f"[{self.channel_name}] ‚ùå Erreur traitement logs FFmpeg: {e}")
            
    def check_for_errors(self):
        """
        V√©rifie les logs pour des erreurs et les traite si n√©cessaire.
        √Ä appeler p√©riodiquement.
        """
        # V√©rifier la taille des logs avant traitement
        self._check_and_rotate_log(self.main_log)
        # Traiter les logs pour d√©tecter les erreurs
        self.process_error_logs()
