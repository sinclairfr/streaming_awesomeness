# iptv_channel.py
import os
import time
import threading
import random
from pathlib import Path
from typing import Optional, List
import shutil
import json
from video_processor import VideoProcessor
from hls_cleaner import HLSCleaner
from ffmpeg_logger import FFmpegLogger
from ffmpeg_command_builder import FFmpegCommandBuilder
from ffmpeg_process_manager import FFmpegProcessManager
from playback_position_manager import PlaybackPositionManager
import subprocess
import re
from config import (
    TIMEOUT_NO_VIEWERS,
    logger,
    CONTENT_DIR,
    USE_GPU,
    CRASH_THRESHOLD,
)
from video_processor import verify_file_ready, get_accurate_duration
import datetime
from error_handler import ErrorHandler


class IPTVChannel:
    """G√®re une cha√Æne IPTV, son streaming et sa surveillance"""

    def __init__(
        self,
        name: str,
        video_dir: str,
        hls_cleaner: HLSCleaner,
        use_gpu: bool = False,
        stats_collector=None,
    ):
        self.name = name
        self.channel_name = name
        self.video_dir = video_dir
        self.use_gpu = use_gpu
        self.hls_cleaner = hls_cleaner
        self.stats_collector = stats_collector

        # Gestion des erreurs int√©gr√©e
        self.error_count = 0
        self.restart_count = 0
        self.max_restarts = 5
        self.restart_cooldown = 60
        self.last_restart_time = 0
        self.error_types = set()
        
        # Initialisation de l'error handler
        self.error_handler = ErrorHandler(
            channel_name=self.name,
            max_restarts=self.max_restarts,
            restart_cooldown=self.restart_cooldown
        )
        
        # Chemin du fichier de log unique pour ce canal
        self.crash_log_path = Path(f"/app/logs/crashes_{self.name}.log")
        self.crash_log_path.parent.mkdir(exist_ok=True)

        self.lock = threading.Lock()
        self.ready_for_streaming = False
        self.total_duration = 0

        # IMPORTANT: Initialiser d'abord le PlaybackPositionManager pour charger les offsets
        self.position_manager = PlaybackPositionManager(name)

        # Ensuite initialiser les autres composants
        self.logger = FFmpegLogger(name)
        self.command_builder = FFmpegCommandBuilder(name, use_gpu=use_gpu)
        self.process_manager = FFmpegProcessManager(name, self.logger)

        # Ajouter cette cha√Æne au registre global
        if hasattr(FFmpegProcessManager, "all_channels"):
            FFmpegProcessManager.all_channels[name] = self

        # Configuration des callbacks
        self.process_manager.on_process_died = self._handle_process_died

        # On ne passe pas directement la m√©thode mais on cr√©e un wrapper
        self.process_manager.on_position_update = self._handle_position_update
        self.process_manager.on_segment_created = self._handle_segment_created

        # Autres composants
        self.processor = VideoProcessor(self.video_dir)

        # Variables de surveillance
        self.processed_videos = []
        self.watchers_count = 0
        self.last_watcher_time = time.time()
        self.last_segment_time = time.time()

        # √âtat du scan initial
        self.initial_scan_complete = False
        self.scan_lock = threading.Lock()

        # Chargement des vid√©os
        logger.info(f"[{self.name}] üîÑ Pr√©paration initiale de la cha√Æne")
        self._scan_videos()
        self._create_concat_file()

        # Calcul de la dur√©e totale et utilisation de l'offset r√©cup√©r√© du fichier JSON
        total_duration = self._calculate_total_duration()
        self.position_manager.set_total_duration(total_duration)
        self.process_manager.set_total_duration(total_duration)

        self.initial_scan_complete = True
        self.ready_for_streaming = len(self.processed_videos) > 0

        logger.info(
            f"[{self.name}] ‚úÖ Initialisation compl√®te. Cha√Æne pr√™te: {self.ready_for_streaming}"
        )

        _playlist_creation_timestamps = {}  # Pour suivre les cr√©ations r√©centes par cha√Æne

    def _create_concat_file(self) -> Optional[Path]:
        """Cr√©e le fichier de concat√©nation avec les bons chemins et sans doublons"""
        # V√©rifier si on a cr√©√© une playlist r√©cemment pour √©viter les doublons
        current_time = time.time()
        if not hasattr(IPTVChannel, "_playlist_creation_timestamps"):
            IPTVChannel._playlist_creation_timestamps = {}

        last_creation = IPTVChannel._playlist_creation_timestamps.get(self.name, 0)
        concat_file = Path(self.video_dir) / "_playlist.txt"

        # Si on a cr√©√© une playlist dans les 10 derni√®res secondes, ne pas recr√©er
        if current_time - last_creation < 10:
            logger.debug(
                f"[{self.name}] ‚ÑπÔ∏è Cr√©ation de playlist ignor√©e (derni√®re: il y a {current_time - last_creation:.1f}s)"
            )
            return concat_file if concat_file.exists() else None

        # Mettre √† jour le timestamp AVANT de cr√©er le fichier
        IPTVChannel._playlist_creation_timestamps[self.name] = current_time
        
        try:
            # Utiliser ready_to_stream au lieu de processed
            ready_to_stream_dir = Path(self.video_dir) / "ready_to_stream"
            if not ready_to_stream_dir.exists():
                logger.error(f"[{self.name}] ‚ùå Dossier ready_to_stream introuvable")
                return None

            # MODIFI√â: On rescanne le dossier ready_to_stream pour √™tre s√ªr d'avoir tous les fichiers
            # au lieu de se baser uniquement sur self.processed_videos qui pourrait ne pas √™tre √† jour
            ready_files = list(ready_to_stream_dir.glob("*.mp4"))
            
            if not ready_files:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 dans {ready_to_stream_dir}")
                return None
                
            logger.info(f"[{self.name}] üîç {len(ready_files)} fichiers trouv√©s dans ready_to_stream")

            # On m√©lange les fichiers pour plus de vari√©t√©
            import random
            random.shuffle(ready_files)

            # V√©rifier que tous les fichiers existent et sont accessibles
            valid_files = []
            for video in ready_files:
                if video.exists() and os.access(video, os.R_OK):
                    # V√©rifier que le fichier est un MP4 valide
                    try:
                        duration = get_accurate_duration(video)
                        if duration and duration > 0:
                            valid_files.append(video)
                        else:
                            logger.warning(f"[{self.name}] ‚ö†Ô∏è Fichier ignor√©: {video.name} (dur√©e invalide)")
                    except Exception as e:
                        logger.warning(f"[{self.name}] ‚ö†Ô∏è Fichier ignor√©: {video.name} (erreur validation: {e})")
                else:
                    logger.warning(f"[{self.name}] ‚ö†Ô∏è Fichier ignor√©: {video.name} (non accessible)")

            if not valid_files:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier valide pour la playlist")
                return None

            logger.info(
                f"[{self.name}] üõ†Ô∏è Cr√©er le fichier de concat√©nation avec {len(valid_files)} fichiers uniques"
            )

            # Cr√©er le fichier de concat√©nation avec une syntaxe simplifi√©e
            with open(concat_file, "w", encoding="utf-8") as f:
                for video in valid_files:
                    escaped_path = str(video.absolute()).replace("'", "'\\''")
                    # Utiliser une syntaxe simple pour la concat√©nation
                    f.write(f"file '{escaped_path}'\n")
                    logger.debug(f"[{self.name}] ‚úÖ Ajout de {video.name}")

            # V√©rifier que le fichier a √©t√© cr√©√© correctement
            if not concat_file.exists() or concat_file.stat().st_size == 0:
                logger.error(f"[{self.name}] ‚ùå Erreur: playlist vide ou non cr√©√©e")
                return None

            # NOUVEAU: Mettre √† jour self.processed_videos pour synchroniser
            self.processed_videos = valid_files.copy()

            logger.info(
                f"[{self.name}] üé• Playlist cr√©√©e avec {len(valid_files)} fichiers uniques en mode al√©atoire"
            )
            return concat_file

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur _playlist.txt: {e}")
            return None

    def _handle_position_update(self, position):
        """Re√ßoit les mises √† jour de position du ProcessManager"""
        try:
            # On se contente de loguer les sauts de position sans red√©marrer
            if hasattr(self, "last_logged_position"):
                if abs(position - self.last_logged_position) > 30:
                    logger.info(f"[{self.name}] üìä Saut d√©tect√©: {self.last_logged_position:.2f}s ‚Üí {position:.2f}s")
                    
                    # V√©rifier si on a des erreurs de DTS
                    if position < self.last_logged_position:
                        logger.warning(f"[{self.name}] ‚ö†Ô∏è DTS non-monotone d√©tect√©")
                        self.last_dts_error_time = time.time()
                        
                        # Si on a trop d'erreurs DTS, on force un red√©marrage
                        if hasattr(self, "dts_error_count"):
                            self.dts_error_count += 1
                            if self.dts_error_count >= 3:
                                logger.error(f"[{self.name}] ‚ùå Trop d'erreurs DTS, red√©marrage forc√©")
                                self.process_manager.restart_process()
                                self.dts_error_count = 0
                        else:
                            self.dts_error_count = 1

            self.last_logged_position = position
            
        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur dans _handle_position_update: {e}")

    def _scan_videos_async(self):
        """Scanne les vid√©os en t√¢che de fond pour les mises √† jour ult√©rieures"""
        # √âviter les ex√©cutions multiples concurrentes
        if hasattr(self, "_scan_in_progress") and self._scan_in_progress:
            logger.debug(f"[{self.name}] ‚è≠Ô∏è Scan d√©j√† en cours, ignor√©")
            return

        # V√©rifier si un scan a √©t√© fait r√©cemment
        current_time = time.time()
        if hasattr(self, "_last_scan_time") and (current_time - self._last_scan_time) < 60:
            logger.debug(f"[{self.name}] ‚è≠Ô∏è Dernier scan trop r√©cent, ignor√©")
            return

        self._scan_in_progress = True
        try:
            with self.scan_lock:
                logger.info(f"[{self.name}] üîç Scan de mise √† jour des vid√©os en cours...")
                old_videos = set(self.processed_videos)
                self._scan_videos()

                # Ne continuer que si des changements ont √©t√© d√©tect√©s
                new_videos = set(self.processed_videos)
                if old_videos == new_videos:
                    logger.debug(f"[{self.name}] ‚ÑπÔ∏è Aucun changement d√©tect√© dans les vid√©os")
                    return

                # Mise √† jour de la dur√©e totale
                total_duration = self._calculate_total_duration()
                self.position_manager.set_total_duration(total_duration)
                self.process_manager.set_total_duration(total_duration)

                # Mise √† jour de la playlist
                self._create_concat_file()

                logger.info(f"[{self.name}] ‚úÖ Scan de mise √† jour termin√©. Cha√Æne pr√™te: {self.ready_for_streaming}")
                self._last_scan_time = current_time

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur scan de mise √† jour: {e}")
        finally:
            self._scan_in_progress = False

    def _calculate_total_duration(self) -> float:
        try:
            # V√©rification que la liste processed_videos n'est pas vide
            if not self.processed_videos:
                logger.warning(
                    f"[{self.name}] ‚ö†Ô∏è Aucune vid√©o √† analyser pour le calcul de dur√©e"
                )
                # On conserve la dur√©e existante si possible, sinon valeur par d√©faut
                if hasattr(self, "total_duration") and self.total_duration > 0:
                    return self.total_duration
                return 3600.0

            # Si la dur√©e a d√©j√† √©t√© calcul√©e et qu'on a le m√™me nombre de fichiers
            # qu'avant, on peut conserver la dur√©e existante pour √©viter les sauts
            existing_duration = getattr(self, "total_duration", 0)
            cached_num_videos = getattr(self, "_num_processed_videos", 0)

            if existing_duration > 0 and cached_num_videos == len(
                self.processed_videos
            ):
                # On v√©rifie si les fichiers sont les m√™mes en comparant les noms
                current_filenames = sorted([p.name for p in self.processed_videos])
                cached_filenames = getattr(self, "_cached_filenames", [])

                if current_filenames == cached_filenames:
                    logger.info(
                        f"[{self.name}] üîÑ Conservation de la dur√©e calcul√©e pr√©c√©demment: {existing_duration:.2f}s"
                    )
                    return existing_duration

            # Calcul de la dur√©e via le position manager avec cache
            total_duration = self.position_manager.calculate_durations(
                self.processed_videos
            )

            if total_duration <= 0:
                logger.warning(
                    f"[{self.name}] ‚ö†Ô∏è Dur√©e totale invalide, fallback √† la valeur existante ou 120s"
                )
                if existing_duration > 0:
                    return existing_duration
                return 3600.0

            # On stocke les m√©tadonn√©es pour les futures comparaisons
            self.total_duration = total_duration
            self._num_processed_videos = len(self.processed_videos)
            self._cached_filenames = sorted([p.name for p in self.processed_videos])

            logger.info(
                f"[{self.name}] ‚úÖ Dur√©e totale calcul√©e: {total_duration:.2f}s ({len(self.processed_videos)} vid√©os)"
            )
            return total_duration

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur calcul dur√©e: {e}")
            # Fallback √† la valeur existante ou valeur par d√©faut
            if hasattr(self, "total_duration") and self.total_duration > 0:
                return self.total_duration
            return 3600.0

    def _check_segments(self, hls_dir: str) -> dict:
        """
        V√©rifie la g√©n√©ration des segments HLS et retourne des donn√©es structur√©es

        Args:
            hls_dir: Chemin du dossier HLS

        Returns:
            dict: Informations sur les segments (count, liste, taille totale)
        """
        try:
            segment_log_path = Path(f"/app/logs/segments/{self.name}_segments.log")
            segment_log_path.parent.mkdir(parents=True, exist_ok=True)

            hls_path = Path(hls_dir)
            playlist = hls_path / "playlist.m3u8"

            if not playlist.exists():
                logger.error(f"[{self.name}] ‚ùå playlist.m3u8 introuvable")
                return {"success": False, "error": "playlist_not_found", "segments": []}

            # Lecture de la playlist
            with open(playlist) as f:
                segments = [line.strip() for line in f if line.strip().endswith(".ts")]

            if not segments:
                logger.warning(f"[{self.name}] ‚ö†Ô∏è Aucun segment dans la playlist")
                return {"success": False, "error": "no_segments", "segments": []}

            # Analyse des segments
            segment_data = []
            total_size = 0
            
            # Variable pour suivre la modification la plus r√©cente
            most_recent_mtime = 0
            
            for segment in segments:
                segment_path = hls_path / segment
                if segment_path.exists():
                    size = segment_path.stat().st_size
                    mtime = segment_path.stat().st_mtime
                    segment_id = (
                        int(segment.split("_")[-1].split(".")[0])
                        if "_" in segment
                        else 0
                    )

                    # Mise √† jour du temps de modification le plus r√©cent
                    if mtime > most_recent_mtime:
                        most_recent_mtime = mtime

                    segment_info = {
                        "name": segment,
                        "size": size,
                        "mtime": mtime,
                        "id": segment_id,
                    }
                    segment_data.append(segment_info)
                    total_size += size
                else:
                    segment_data.append(
                        {
                            "name": segment,
                            "missing": True,
                            "id": (
                                int(segment.split("_")[-1].split(".")[0])
                                if "_" in segment
                                else 0
                            ),
                        }
                    )

            # Tri des segments par ID
            segment_data.sort(key=lambda x: x.get("id", 0))

            # Mettre √† jour last_segment_time si un segment r√©cent a √©t√© d√©tect√©
            if most_recent_mtime > 0:
                current_time = time.time()
                segment_age = current_time - most_recent_mtime
                
                # Si un segment r√©cent a √©t√© d√©tect√©, mise √† jour du timestamp 
                if segment_age < CRASH_THRESHOLD:
                    logger.debug(f"[{self.name}] ‚úÖ Segment r√©cent trouv√© (√¢ge: {segment_age:.1f}s), mise √† jour du last_segment_time")
                    self.last_segment_time = current_time

            # D√©tection des sauts de segments
            jumps = []
            for i in range(1, len(segment_data)):
                current_id = segment_data[i].get("id", 0)
                prev_id = segment_data[i - 1].get("id", 0)
                if current_id - prev_id > 5:  # Saut de plus de 5 segments
                    jumps.append(
                        {"from": prev_id, "to": current_id, "gap": current_id - prev_id}
                    )
                    logger.warning(
                        f"[{self.name}] üö® Saut de segment d√©tect√©: {prev_id} ‚Üí {current_id} (saut de {current_id - prev_id})"
                    )

            # Log des informations
            current_time = time.strftime("%Y-%m-%d %H:%M:%S")
            log_entry = f"{current_time} - Segments actifs: {len(segments)}, Taille totale: {total_size/1024:.1f}KB\n"

            for seg in segment_data:
                if "missing" in seg:
                    log_entry += f"  - {seg['name']} (MANQUANT)\n"
                else:
                    mtime_str = time.strftime("%H:%M:%S", time.localtime(seg["mtime"]))
                    log_entry += f"  - {seg['name']} (ID: {seg['id']}, Taille: {seg['size']/1024:.1f}KB, Modifi√©: {mtime_str})\n"

            if jumps:
                log_entry += f"  - SAUTS D√âTECT√âS: {len(jumps)}\n"
                for jump in jumps:
                    log_entry += f"    * Saut de {jump['from']} √† {jump['to']} (gap: {jump['gap']})\n"

            with open(segment_log_path, "a") as f:
                f.write(log_entry)
                f.write("-" * 80 + "\n")

            return {
                "success": True,
                "count": len(segments),
                "segments": segment_data,
                "total_size": total_size,
                "jumps": jumps,
            }

        except Exception as e:
            logger.error(f"[{self.name}] Erreur analyse segments: {e}")
            return {"success": False, "error": str(e), "segments": []}

    def _handle_timeouts(self, current_time=None, crash_threshold=None):
        """
        G√®re les timeouts et red√©marre le stream si n√©cessaire

        Args:
            current_time: Temps actuel (calcul√© automatiquement si None)
            crash_threshold: Seuil en secondes pour consid√©rer un crash (utilise CRASH_THRESHOLD si None)

        Returns:
            bool: True si action entreprise, False sinon
        """
        # Si pas de temps fourni, on prend le temps actuel
        if current_time is None:
            current_time = time.time()

        # Utiliser le seuil fourni ou celui de l'environnement
        if crash_threshold is None:
            crash_threshold = CRASH_THRESHOLD

        # On v√©rifie que last_segment_time existe
        if not hasattr(self, "last_segment_time"):
            self.last_segment_time = current_time
            return False

        # On v√©rifie si on a un timeout de segments
        time_since_last_segment = current_time - self.last_segment_time

        if time_since_last_segment > crash_threshold:
            logger.error(
                f"[{self.name}] üî• Pas de nouveau segment depuis {time_since_last_segment:.1f}s "
                f"(seuil: {crash_threshold}s)"
            )

            # V√©rifier l'√©tat du processus FFmpeg
            if not self.process_manager.is_running():
                logger.warning(f"[{self.name}] ‚ö†Ô∏è Processus FFmpeg d√©j√† arr√™t√©")
                # On recr√©e le stream seulement s'il y a des viewers
                if hasattr(self, "watchers_count") and self.watchers_count > 0:
                    logger.info(
                        f"[{self.name}] üîÑ Red√©marrage du stream (viewers actifs: {self.watchers_count})"
                    )
                    if self._restart_stream():
                        self.error_handler.reset()
                        return True
                return False

            # Analyse des segments actuels pour diagnostiquer
            hls_dir = f"/app/hls/{self.name}"
            segments_info = self._check_segments(hls_dir)

            # D√©tection des causes possibles
            if segments_info.get("success"):
                jumps = segments_info.get("jumps", [])
                if jumps and len(jumps) > 0:
                    logger.error(
                        f"[{self.name}] üö® Probl√®me possible: sauts de segments d√©tect√©s ({len(jumps)})"
                    )
                    # On signale les sauts pour le monitoring
                    self.report_segment_jump(jumps[0]["from"], jumps[0]["to"])

            # On utilise l'error handler pour g√©rer les red√©marrages
            if self.error_handler.add_error("segment_timeout"):
                logger.info(f"[{self.name}] üîÑ Red√©marrage apr√®s timeout de segment")
                if self._restart_stream():
                    self.error_handler.reset()
                    return True

            return True

        return False

    def report_segment_jump(self, prev_segment: int, curr_segment: int):
        """
        G√®re les sauts d√©tect√©s dans les segments HLS avec historique et prise de d√©cision

        Args:
            prev_segment: Le segment pr√©c√©dent
            curr_segment: Le segment actuel (avec un saut)
        """
        try:
            jump_size = curr_segment - prev_segment

            # On ne s'inqui√®te que des sauts importants
            if jump_size <= 3:  # Tol√©rance pour quelques segments perdus
                return

            logger.warning(
                f"[{self.name}] üö® Saut de segment d√©tect√©: {prev_segment} ‚Üí {curr_segment} (delta: {jump_size})"
            )

            # On stocke l'historique des sauts
            if not hasattr(self, "jump_history"):
                self.jump_history = []

            # Ajout du saut √† l'historique avec timestamp
            current_time = time.time()
            self.jump_history.append(
                {
                    "time": current_time,
                    "from": prev_segment,
                    "to": curr_segment,
                    "gap": jump_size,
                }
            )

            # On ne garde que les 10 derniers sauts
            if len(self.jump_history) > 10:
                self.jump_history = self.jump_history[-10:]

            # Analyse des sauts r√©cents (5 derni√®res minutes)
            recent_jumps = [
                j for j in self.jump_history if current_time - j["time"] < 300
            ]

            # D√©tection des sch√©mas de sauts
            if len(recent_jumps) >= 3:
                # Sauts de taille similaire (¬± 20%)
                similar_sized_jumps = []
                for jump in recent_jumps:
                    similar = [
                        j
                        for j in recent_jumps
                        if abs(j["gap"] - jump["gap"]) / max(1, jump["gap"]) < 0.2
                    ]
                    if len(similar) >= 3:
                        similar_sized_jumps = similar
                        break

                # Si on a des sauts similaires, c'est probablement un probl√®me syst√©mique
                if similar_sized_jumps:
                    avg_gap = sum(j["gap"] for j in similar_sized_jumps) / len(
                        similar_sized_jumps
                    )
                    logger.error(
                        f"[{self.name}] üî• Probl√®me syst√©mique d√©tect√©: {len(similar_sized_jumps)} sauts "
                        f"avec gap moyen de {avg_gap:.1f} segments"
                    )

                    # On red√©marre seulement s'il y a des viewers actifs
                    if hasattr(self, "watchers_count") and self.watchers_count > 0:
                        if self.error_handler.add_error(
                            f"segment_jumps_{int(avg_gap)}"
                        ):
                            logger.warning(
                                f"[{self.name}] üîÑ Red√©marrage programm√© apr√®s d√©tection de sauts syst√©miques"
                            )
                            return self._restart_stream()
                    else:
                        logger.info(
                            f"[{self.name}] ‚ÑπÔ∏è Pas de red√©marrage apr√®s sauts: aucun viewer actif"
                        )

            return False

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur gestion saut de segment: {e}")
            return False

    def _check_viewer_inactivity(self, current_time, timeout):
        """V√©rifie l'inactivit√© des viewers et g√®re l'arr√™t du stream"""
        if not self.process_manager.is_running():
            return False

        inactivity_duration = current_time - self.last_watcher_time

        if inactivity_duration > timeout + 60:
            logger.info(
                f"[{self.name}] ‚ö†Ô∏è Inactivit√© d√©tect√©e: {inactivity_duration:.1f}s"
            )
            return True

        return False

    def _clean_processes(self):
        """Nettoie les processus en utilisant le ProcessManager"""
        try:
            self.process_manager.stop_process()
            logger.info(f"[{self.name}] üßπ Nettoyage des processus termin√©")
        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur nettoyage processus: {e}")

    def _check_and_move_invalid_files(self):
        """V√©rifie les fichiers dans ready_to_stream et d√©place les invalides vers ignored"""
        try:
            ready_dir = Path(self.video_dir) / "ready_to_stream"
            ignored_dir = Path(self.video_dir) / "ignored"
            ignored_dir.mkdir(exist_ok=True)

            for video in ready_dir.glob("*.mp4"):
                # On tente de valider le fichier avec ffprobe
                is_valid = verify_file_ready(video)

                if not is_valid:
                    # On d√©place vers le dossier ignored
                    dest = ignored_dir / video.name
                    reason_file = ignored_dir / f"{video.stem}_reason.txt"

                    # Si le fichier de destination existe d√©j√†, on le supprime
                    if dest.exists():
                        dest.unlink()

                    # On d√©place le fichier
                    shutil.move(str(video), str(dest))

                    # On √©crit la raison
                    with open(reason_file, "w") as f:
                        f.write(
                            f"Fichier ignor√© le {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                        )
                        f.write(
                            "Raison: Fichier invalide pour le streaming (v√©rification √©chou√©e)\n"
                        )

                    logger.warning(
                        f"[{self.name}] üö´ Fichier {video.name} invalide d√©plac√© vers ignored"
                    )

            return True
        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur v√©rification fichiers: {e}")
            return False

    def _verify_playlist(self):
        """V√©rifie que le fichier playlist est valide"""
        try:
            playlist_path = Path(self.video_dir) / "_playlist.txt"
            if not playlist_path.exists():
                logger.error(f"[{self.channel_name}] ‚ùå _playlist.txt n'existe pas")
                return False

            with open(playlist_path, "r") as f:
                lines = f.readlines()

            if not lines:
                logger.error(f"[{self.name}] ‚ùå _playlist.txt est vide")
                return False

            valid_count = 0
            for i, line in enumerate(lines, 1):
                line = line.strip()
                if not line:
                    continue

                if not line.startswith("file"):
                    logger.error(f"[{self.name}] ‚ùå Ligne {i} invalide: {line}")
                    return False

                try:
                    file_path = line.split("'")[1] if "'" in line else line.split()[1]
                    file_path = Path(file_path)
                    if not file_path.exists():
                        logger.error(f"[{self.name}] ‚ùå Fichier manquant: {file_path}")
                        return False
                    valid_count += 1
                except Exception as e:
                    logger.error(f"[{self.name}] ‚ùå Erreur parsing ligne {i}: {e}")
                    return False

            if valid_count == 0:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier valide dans la playlist")
                return False

            logger.info(f"[{self.name}] ‚úÖ Playlist valide avec {valid_count} fichiers")
            return True

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur v√©rification playlist: {e}")
            return False

    def _monitor_playlist_transition(self):
        """Surveille la transition entre les fichiers de la playlist"""
        try:
            if not self.process_manager.is_running():
                return

            # V√©rifier la playlist actuelle
            playlist_path = Path(self.video_dir) / "_playlist.txt"
            if not playlist_path.exists():
                logger.error(f"[{self.name}] ‚ùå Playlist introuvable")
                return

            # Lire la playlist
            with open(playlist_path, "r") as f:
                lines = f.readlines()
                playlist_files = [line.strip().split("'")[1] for line in lines if line.strip().startswith("file")]

            if not playlist_files:
                logger.error(f"[{self.name}] ‚ùå Playlist vide")
                return

            # V√©rifier que tous les fichiers existent
            missing_files = []
            for file_path in playlist_files:
                if not Path(file_path).exists():
                    missing_files.append(file_path)

            if missing_files:
                logger.error(f"[{self.name}] ‚ùå Fichiers manquants dans la playlist: {missing_files}")
                # Recr√©er la playlist sans les fichiers manquants
                self._create_concat_file()
                return

            # V√©rifier la position actuelle de lecture
            if hasattr(self, "last_logged_position"):
                current_position = self.last_logged_position
            else:
                return

            # V√©rifier les erreurs segment dans les logs nginx
            hls_dir = Path(f"/app/hls/{self.name}")
            playlist_file = hls_dir / "playlist.m3u8"
            
            # V√©rifier si on est proche de la fin d'un fichier
            for file_path in playlist_files:
                duration = get_accurate_duration(Path(file_path))
                if duration and current_position >= duration - 30:  # Augment√© √† 30 secondes pour plus de marge
                    logger.info(f"[{self.name}] üîÑ Transition imminente vers le fichier suivant: {Path(file_path).name}")
                    
                    # V√©rifier l'int√©grit√© des segments r√©cents
                    segments_info = self._check_segments(str(hls_dir))
                    has_segment_issues = False
                    
                    if segments_info.get("success"):
                        # V√©rifier les sauts de segments
                        if segments_info.get("jumps"):
                            has_segment_issues = True
                            logger.warning(f"[{self.name}] üö® Sauts de segments d√©tect√©s pendant la transition")
                        
                        # V√©rifier s'il y a des segments manquants
                        missing_segments = [s for s in segments_info.get("segments", []) if s.get("missing", False)]
                        if missing_segments:
                            has_segment_issues = True
                            logger.warning(f"[{self.name}] üö® Segments manquants pendant la transition: {len(missing_segments)}")
                    
                    # V√©rifier si on a des erreurs de DTS
                    if hasattr(self, "last_dts_error_time"):
                        if time.time() - self.last_dts_error_time < 60:  # Si on a eu une erreur DTS r√©cente
                            has_segment_issues = True
                            logger.warning(f"[{self.name}] ‚ö†Ô∏è Erreur DTS r√©cente d√©tect√©e pendant la transition")
                    
                    # Si on a d√©tect√© des probl√®mes ou on est √† moins de 5 secondes de la fin,
                    # on force un red√©marrage propre pour √©viter les saccades
                    if has_segment_issues or current_position >= duration - 5:
                        logger.warning(f"[{self.name}] üîÑ Red√©marrage pr√©ventif du stream pour assurer une transition fluide")
                        if self.process_manager.is_running():
                            # On sauvegarde la playlist actuelle pour la restaurer apr√®s red√©marrage
                            if playlist_file.exists():
                                try:
                                    playlist_backup = str(playlist_file) + ".backup"
                                    with open(playlist_file, "r") as src, open(playlist_backup, "w") as dest:
                                        dest.write(src.read())
                                except Exception as e:
                                    logger.error(f"[{self.name}] ‚ùå Erreur sauvegarde playlist: {e}")
                            
                            # Red√©marrage du processus FFmpeg
                            self.process_manager.restart_process()
                            
                            # Ajout d'un d√©lai pour s'assurer que FFmpeg a le temps de d√©marrer
                            time.sleep(3)
                            
                            # Forcer une mise √† jour de la playlist
                            self._create_concat_file()
                            return
                    
                    # Forcer une mise √† jour de la playlist
                    self._create_concat_file()
                    # Ajouter un d√©lai pour s'assurer que FFmpeg a le temps de g√©rer la transition
                    time.sleep(3)  # Augment√© √† 3 secondes
                    break

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur surveillance transition playlist: {e}")

    def _segment_monitor_loop(self):
        """Boucle de surveillance des segments"""
        try:
            # La boucle s'ex√©cute tant que le processus est en cours
            while self.process_manager.is_running():
                current_time = time.time()

                # V√©rification des timeouts toutes les 10 secondes
                self._handle_timeouts(current_time)

                # Health check toutes les 30 secondes
                if not hasattr(self, "last_health_check") or current_time - self.last_health_check >= 30:
                    self.channel_health_check()
                    self.last_health_check = current_time

                # V√©rification directe des segments dans le syst√®me de fichiers
                self._check_filesystem_segments()

                # Analyse des segments toutes les 30 secondes
                if (
                    not hasattr(self, "last_segment_check")
                    or current_time - self.last_segment_check >= 30
                ):
                    hls_dir = f"/app/hls/{self.name}"
                    segments_info = self._check_segments(hls_dir)
                    self.last_segment_check = current_time

                    # Si on a d√©tect√© des sauts, on les reporte
                    if segments_info.get("success") and segments_info.get("jumps"):
                        for jump in segments_info.get("jumps", []):
                            self.report_segment_jump(jump["from"], jump["to"])

                # V√©rification de la transition playlist toutes les 5 secondes
                if (
                    not hasattr(self, "last_playlist_check")
                    or current_time - self.last_playlist_check >= 5
                ):
                    self._monitor_playlist_transition()
                    self.last_playlist_check = current_time

                # On attend un peu avant la prochaine v√©rification
                time.sleep(1)

        except Exception as e:
            logger.error(
                f"[{self.name}] ‚ùå Erreur dans la boucle de surveillance des segments: {e}"
            )

    def _check_filesystem_segments(self):
        """
        V√©rifie directement les fichiers segment sur le disque pour d√©tecter les cr√©ations r√©centes
        ind√©pendamment de la playlist. Cela r√©sout les probl√®mes de d√©tection de timeouts.
        """
        try:
            current_time = time.time()
            
            # Ne v√©rifier que toutes les 5 secondes pour limiter les acc√®s disque
            if hasattr(self, "last_fs_check") and current_time - self.last_fs_check < 5:
                return
                
            self.last_fs_check = current_time
            
            # Chemin du dossier HLS
            hls_path = Path(f"/app/hls/{self.name}")
            if not hls_path.exists():
                return
                
            # Trouver tous les segments .ts
            segments = list(hls_path.glob("segment_*.ts"))
            if not segments:
                return
                
            # Trouver le segment le plus r√©cent
            most_recent = max(segments, key=lambda s: s.stat().st_mtime)
            if not most_recent:
                return
                
            # V√©rifier l'√¢ge du segment le plus r√©cent
            mtime = most_recent.stat().st_mtime
            segment_age = current_time - mtime
            
            # Plus permissif: consid√©rer un segment comme r√©cent si son √¢ge est < CRASH_THRESHOLD * 3
            # au lieu de CRASH_THRESHOLD
            relaxed_threshold = CRASH_THRESHOLD * 3
            
            # Si le segment est r√©cent, mettre √† jour le timestamp
            if segment_age < relaxed_threshold:
                # Utiliser debug_fs=True pour √©viter de spammer les logs
                debug_fs = getattr(self, 'debug_fs', False)
                if debug_fs or len(segments) % 10 == 0:  # Uniquement log tous les 10 segments ou en mode debug
                    logger.debug(
                        f"[{self.name}] üÜï Segment r√©cent trouv√©: {most_recent.name} "
                        f"(√¢ge: {segment_age:.1f}s, seuil: {relaxed_threshold}s, total segments: {len(segments)})"
                    )
                
                # Toujours mettre √† jour le timestamp de segment pour √©viter les faux timeouts
                self.last_segment_time = current_time
                
        except Exception as e:
            logger.debug(f"[{self.name}] Erreur v√©rification FS segments: {e}")
            
    def _restart_stream(self) -> bool:
        """Red√©marre le stream en cas de probl√®me avec une meilleure gestion de la continuit√©"""
        try:
            logger.info(f"üîÑ Red√©marrage du stream {self.name}")

            # Ajoute un d√©lai al√©atoire pour √©viter les red√©marrages en cascade
            jitter = random.uniform(0.5, 2.0)
            time.sleep(jitter)

            elapsed = time.time() - getattr(self, "last_restart_time", 0)
            if elapsed < self.error_handler.restart_cooldown:
                logger.info(
                    f"‚è≥ Attente du cooldown ({self.error_handler.restart_cooldown - elapsed:.1f}s)"
                )
                time.sleep(self.error_handler.restart_cooldown - elapsed)

            self.last_restart_time = time.time()

            # 1. Arr√™ter proprement le streaming s√©quentiel actuel
            if hasattr(self, "_streaming_active") and self._streaming_active:
                logger.info(f"[{self.name}] üõë Arr√™t du streaming s√©quentiel actuel")
                self._streaming_active = False
                
                # Attendre que le thread de streaming se termine
                if hasattr(self, "_streaming_thread") and self._streaming_thread.is_alive():
                    self._streaming_thread.join(timeout=5)
                    logger.info(f"[{self.name}] ‚úÖ Thread de streaming arr√™t√©")

            # 2. Arr√™ter le processus FFmpeg en cours si pr√©sent
            if hasattr(self, "_current_process") and self._current_process:
                try:
                    self._current_process.terminate()
                    self._current_process.wait(timeout=5)
                    logger.info(f"[{self.name}] ‚úÖ Processus FFmpeg arr√™t√©")
                except Exception as e:
                    logger.warning(f"[{self.name}] ‚ö†Ô∏è Erreur arr√™t processus FFmpeg: {e}")
                    try:
                        self._current_process.kill()
                    except:
                        pass

            # 3. Nettoyer le dossier HLS
            hls_dir = Path(f"/app/hls/{self.name}")
            if hls_dir.exists():
                for old_file in hls_dir.glob("*.ts"):
                    try:
                        old_file.unlink()
                    except Exception as e:
                        logger.debug(
                            f"[{self.name}] Impossible de supprimer {old_file}: {e}"
                        )

                # Supprimer aussi l'ancienne playlist
                old_playlist = hls_dir / "playlist.m3u8"
                if old_playlist.exists():
                    try:
                        old_playlist.unlink()
                    except Exception:
                        pass

            # 4. V√©rifier que nous avons des fichiers valides
            ready_dir = Path(self.video_dir) / "ready_to_stream"
            if not ready_dir.exists():
                logger.error(f"[{self.name}] ‚ùå Dossier ready_to_stream introuvable")
                return False

            video_files = list(ready_dir.glob("*.mp4"))
            if not video_files:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 dans ready_to_stream")
                return False

            # V√©rifier que les fichiers sont valides
            valid_files = []
            for video_file in video_files:
                if verify_file_ready(video_file):
                    valid_files.append(video_file)
                else:
                    logger.warning(f"[{self.name}] ‚ö†Ô∏è Fichier {video_file.name} ignor√© car non valide")

            if not valid_files:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 valide trouv√©")
                return False

            # 5. Lancer un nouveau stream frais
            return self.start_stream()

        except Exception as e:
            logger.error(f"Erreur lors du red√©marrage de {self.name}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def stop_stream_if_needed(self):
        """Arr√™te le stream si n√©cessaire"""
        try:
            # Arr√™ter le streaming s√©quentiel si actif
            if hasattr(self, "_streaming_active") and self._streaming_active:
                logger.info(f"[{self.name}] üõë Arr√™t du streaming s√©quentiel")
                self._streaming_active = False
                
                # Attendre que le thread de streaming se termine
                if hasattr(self, "_streaming_thread") and self._streaming_thread.is_alive():
                    self._streaming_thread.join(timeout=5)
                    logger.info(f"[{self.name}] ‚úÖ Thread de streaming arr√™t√©")

            # Arr√™ter le processus FFmpeg en cours si pr√©sent
            if hasattr(self, "_current_process") and self._current_process:
                try:
                    self._current_process.terminate()
                    self._current_process.wait(timeout=5)
                    logger.info(f"[{self.name}] ‚úÖ Processus FFmpeg arr√™t√©")
                except Exception as e:
                    logger.warning(f"[{self.name}] ‚ö†Ô∏è Erreur arr√™t processus FFmpeg: {e}")
                    try:
                        self._current_process.kill()
                    except:
                        pass

            # Nettoyer le dossier HLS
            hls_dir = Path(f"/app/hls/{self.name}")
            if hls_dir.exists():
                for seg in hls_dir.glob("*.ts"):
                    try:
                        seg.unlink()
                    except Exception as e:
                        logger.warning(f"[{self.name}] ‚ö†Ô∏è Erreur suppression segment {seg.name}: {e}")

                # Supprimer la playlist
                playlist = hls_dir / "playlist.m3u8"
                if playlist.exists():
                    try:
                        playlist.unlink()
                    except Exception as e:
                        logger.warning(f"[{self.name}] ‚ö†Ô∏è Erreur suppression playlist: {e}")

            logger.info(f"[{self.name}] ‚úÖ Stream arr√™t√© avec succ√®s")
            return True

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur arr√™t stream: {e}")
            return False

    def start_stream_if_needed(self):
        """D√©marre le stream uniquement s'il n'est pas d√©j√† en cours"""
        try:
            # V√©rifier si le streaming s√©quentiel est d√©j√† actif
            if hasattr(self, "_streaming_active") and self._streaming_active:
                logger.info(f"[{self.name}] ‚úì Streaming s√©quentiel d√©j√† actif, aucune action requise")
                return True

            # V√©rifier si la cha√Æne est pr√™te avant de d√©marrer
            if not self.ready_for_streaming:
                # V√©rifier si des vid√©os sont disponibles m√™me si ready_for_streaming est False
                ready_dir = Path(self.video_dir) / "ready_to_stream"
                has_videos = False

                if ready_dir.exists():
                    video_files = list(ready_dir.glob("*.mp4"))
                    has_videos = len(video_files) > 0

                    if has_videos:
                        logger.info(
                            f"[{self.name}] üîç Vid√©os d√©tect√©es dans ready_to_stream mais ready_for_streaming=False, tentative d'activation"
                        )
                        self.ready_for_streaming = True
                        # Recalcul de la dur√©e totale en arri√®re-plan
                        threading.Thread(
                            target=self._calculate_total_duration, daemon=True
                        ).start()
                    else:
                        logger.warning(
                            f"[{self.name}] ‚ö†Ô∏è Aucune vid√©o disponible, d√©marrage impossible"
                        )
                        return False
                else:
                    logger.warning(
                        f"[{self.name}] ‚ö†Ô∏è Dossier ready_to_stream inexistant, d√©marrage impossible"
                    )
                    return False

            # V√©rifier si nous avons des fichiers MP4 dans ready_to_stream
            ready_dir = Path(self.video_dir) / "ready_to_stream"
            if not ready_dir.exists():
                logger.error(f"[{self.name}] ‚ùå Dossier ready_to_stream introuvable")
                return False

            video_files = list(ready_dir.glob("*.mp4"))
            if not video_files:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 dans ready_to_stream")
                return False

            # V√©rifier que les fichiers sont valides
            valid_files = []
            for video_file in video_files:
                if verify_file_ready(video_file):
                    valid_files.append(video_file)
                else:
                    logger.warning(f"[{self.name}] ‚ö†Ô∏è Fichier {video_file.name} ignor√© car non valide")

            if not valid_files:
                logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 valide trouv√©")
                return False

            # Tout est bon, on d√©marre le stream
            logger.info(f"[{self.name}] üöÄ D√©marrage automatique du streaming s√©quentiel")
            return self.start_stream()

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur start_stream_if_needed: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def channel_health_check(self):
        """Checks if the channel is healthy and running, restarts if needed"""
        try:
            # Increment health check counter for logging
            if not hasattr(self, "_health_check_count"):
                self._health_check_count = 0
            self._health_check_count += 1
            
            # Add startup grace period tracking
            if not hasattr(self, "_startup_time"):
                self._startup_time = time.time()
            
            current_time = time.time()
            startup_duration = current_time - self._startup_time
            
            logger.info(f"[{self.name}] ü©∫ Health check #{self._health_check_count} - Stream running: {self.process_manager.is_running()}, Watchers: {getattr(self, 'watchers_count', 0)}, Startup duration: {startup_duration:.1f}s")
            
            # 1. Check if the process is running
            if not self.process_manager.is_running():
                logger.warning(f"[{self.name}] ‚ö†Ô∏è Stream process not running but should be (watchers: {getattr(self, 'watchers_count', 0)})")
                
                # If we have active watchers, restart the stream
                if hasattr(self, "watchers_count") and self.watchers_count > 0:
                    logger.info(f"[{self.name}] üîÑ Auto-restarting stream due to active watchers ({self.watchers_count})")
                    return self.start_stream()
                return False
            
            # 2. Check if HLS directory has segments and they're recent
            hls_dir = Path(f"/app/hls/{self.name}")
            segments = list(hls_dir.glob("*.ts")) if hls_dir.exists() else []
            
            # Check playlist.m3u8 with startup grace period
            playlist_path = hls_dir / "playlist.m3u8"
            if not playlist_path.exists():
                # During startup grace period, be more lenient with playlist generation
                if startup_duration < 5:  # R√©duit de 15s √† 5s
                    logger.info(f"[{self.name}] ‚ÑπÔ∏è Still in startup grace period ({startup_duration:.1f}s), waiting for playlist.m3u8...")
                    return True
                else:
                    logger.error(f"[{self.name}] ‚ùå playlist.m3u8 introuvable apr√®s {startup_duration:.1f}s")
            
            if not segments:
                logger.warning(f"[{self.name}] ‚ö†Ô∏è No segments found even though process is running")
                
                # Check if playlist.m3u8 exists and is valid
                if playlist_path.exists():
                    try:
                        with open(playlist_path, 'r') as f:
                            playlist_content = f.read()
                            if "#EXTM3U" in playlist_content:
                                logger.info(f"[{self.name}] ‚ÑπÔ∏è playlist.m3u8 exists and is valid, waiting for segments...")
                                return True
                    except Exception as e:
                        logger.error(f"[{self.name}] ‚ùå Error reading playlist.m3u8: {e}")
                
                # During startup grace period, be more lenient
                if startup_duration < 5:  # R√©duit de 15s √† 5s
                    logger.info(f"[{self.name}] ‚ÑπÔ∏è Still in startup grace period ({startup_duration:.1f}s), waiting for segments...")
                    return True
                
                # If we have active watchers and outside grace period, restart the stream
                if hasattr(self, "watchers_count") and self.watchers_count > 0:
                    logger.info(f"[{self.name}] üîÑ Auto-restarting stream due to missing segments")
                    return self.start_stream()
                return False

            # 3. Check for segment continuity and missing segments
            if playlist_path.exists():
                try:
                    # Analyser le contenu de la playlist pour d√©tecter les segments r√©f√©renc√©s
                    with open(playlist_path, "r") as f:
                        playlist_content = f.readlines()
                    
                    referenced_segments = []
                    for line in playlist_content:
                        if line.strip().endswith(".ts"):
                            segment_name = line.strip()
                            referenced_segments.append(segment_name)
                    
                    # V√©rifier que chaque segment r√©f√©renc√© existe bien
                    missing_segments = []
                    for segment_name in referenced_segments:
                        segment_path = hls_dir / segment_name
                        if not segment_path.exists():
                            missing_segments.append(segment_name)
                    
                    # Si des segments r√©f√©renc√©s sont manquants, signaler l'anomalie
                    if missing_segments and len(missing_segments) > 0:
                        # Si plus de 30% des segments sont manquants, c'est un probl√®me critique
                        missing_percent = (len(missing_segments) / len(referenced_segments)) * 100
                        logger.warning(f"[{self.name}] üö® Segments manquants: {len(missing_segments)}/{len(referenced_segments)} ({missing_percent:.1f}%)")
                        
                        if missing_percent > 30 and hasattr(self, "watchers_count") and self.watchers_count > 0:
                            logger.error(f"[{self.name}] ‚ùå Trop de segments manquants ({missing_percent:.1f}%), red√©marrage du stream")
                            self.stop_stream_if_needed()
                            time.sleep(2)
                            return self.start_stream()
                except Exception as e:
                    logger.error(f"[{self.name}] ‚ùå Erreur analyse playlist: {e}")
            
            # Check segment age with different thresholds based on startup phase
            if segments:
                # Extract segment numbers and sort them
                segment_numbers = []
                for seg in segments:
                    try:
                        # Extract number from filename (e.g., "segment_123.ts" -> 123)
                        num = int(seg.stem.split('_')[1])
                        segment_numbers.append((num, seg))
                    except (ValueError, IndexError):
                        continue
                
                if segment_numbers:
                    # Sort by segment number
                    segment_numbers.sort(key=lambda x: x[0])
                    
                    # Get the latest segment number and file
                    latest_num, latest_segment = segment_numbers[-1]
                    
                    # V√©rifier les sauts de num√©rotation des segments
                    segment_jumps = []
                    for i in range(1, len(segment_numbers)):
                        current_num = segment_numbers[i][0]
                        prev_num = segment_numbers[i-1][0]
                        gap = current_num - prev_num
                        if gap > 2:  # Consid√©rer un saut si l'√©cart est > 2
                            segment_jumps.append((prev_num, current_num, gap))
                    
                    # Si on d√©tecte des sauts importants dans la num√©rotation des segments r√©cents
                    if segment_jumps:
                        jump_str = ", ".join([f"{prev}->{curr} (gap:{gap})" for prev, curr, gap in segment_jumps])
                        logger.warning(f"[{self.name}] üö® Sauts de segments d√©tect√©s: {jump_str}")
                        
                        # Si on a des watchers et des sauts importants, red√©marrer
                        if hasattr(self, "watchers_count") and self.watchers_count > 0:
                            # V√©rifier si les sauts sont r√©cents (parmi les derniers segments)
                            recent_jumps = [j for j in segment_jumps if j[1] >= latest_num - 10]
                            if recent_jumps:
                                logger.error(f"[{self.name}] ‚ùå Sauts r√©cents d√©tect√©s, red√©marrage du stream")
                                self.stop_stream_if_needed()
                                time.sleep(2)
                                return self.start_stream()
                    
                    # Check if we have a reasonable number of segments
                    if len(segment_numbers) >= 2:
                        # Get the second latest segment
                        second_latest_num, second_latest_segment = segment_numbers[-2]
                        
                        # Calculate the gap between segments
                        gap = latest_num - second_latest_num
                        
                        # If gap is reasonable (1 or 2), use the latest segment
                        if gap <= 2:
                            segment_age = current_time - latest_segment.stat().st_mtime
                        else:
                            # If gap is too large, use the second latest segment
                            segment_age = current_time - second_latest_segment.stat().st_mtime
                    else:
                        # If we only have one segment, use it
                        segment_age = current_time - latest_segment.stat().st_mtime
                    
                    # More lenient threshold during startup
                    threshold = 60 if startup_duration < 60 else 120
                    
                    if segment_age > threshold:
                        logger.warning(f"[{self.name}] ‚ö†Ô∏è Segments too old (last: {segment_age:.1f}s ago, threshold: {threshold}s)")
                        
                        # During startup grace period, be more lenient
                        if startup_duration < 120:  # Extended from 60s to 120s
                            logger.info(f"[{self.name}] ‚ÑπÔ∏è Still in startup grace period ({startup_duration:.1f}s), waiting for segments...")
                            return True
                        
                        # Restart if there are watchers and outside grace period
                        # Ajout d'une v√©rification suppl√©mentaire: uniquement red√©marrer si les segments sont vraiment vieux
                        if hasattr(self, "watchers_count") and self.watchers_count > 0 and segment_age > threshold * 1.5:  # 50% plus vieux que le seuil
                            logger.info(f"[{self.name}] üîÑ Auto-restarting stream due to stale segments")
                            self.stop_stream_if_needed()
                            time.sleep(2)
                            return self.start_stream()
            
            return True
            
        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Error in health check: {e}")
            return False

    def _verify_processor(self) -> bool:
        """V√©rifie que le VideoProcessor est correctement initialis√©"""
        try:
            if not self.processor:
                logger.error(f"[{self.name}] ‚ùå VideoProcessor non initialis√©")
                return False

            video_dir = Path(self.video_dir)
            if not video_dir.exists():
                logger.error(f"[{self.name}] ‚ùå Dossier vid√©o introuvable: {video_dir}")
                return False

            if not os.access(video_dir, os.R_OK | os.W_OK):
                logger.error(
                    f"[{self.name}] ‚ùå Permissions insuffisantes sur {video_dir}"
                )
                return False

            ready_to_stream_dir = video_dir / "ready_to_stream"
            if not ready_to_stream_dir.exists():
                try:
                    ready_to_stream_dir.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    logger.error(
                        f"[{self.name}] ‚ùå Impossible de cr√©er {ready_to_stream_dir}: {e}"
                    )
                    return False

            logger.debug(f"[{self.name}] ‚úÖ VideoProcessor correctement initialis√©")
            return True

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur v√©rification VideoProcessor: {e}")
            return False

    def _handle_process_died(self, exit_code: int, stderr: str):
        """G√®re la mort du processus FFmpeg"""
        try:
            # Analyser l'erreur
            error_type = self._analyze_error(stderr)
            
            # Utiliser la nouvelle gestion d'erreurs int√©gr√©e
            if self.add_error(error_type):
                logger.warning(f"[{self.name}] Red√©marrage n√©cessaire apr√®s erreur: {error_type}")
                self._restart_stream()
            elif self.has_critical_errors():
                logger.error(f"[{self.name}] Erreurs critiques d√©tect√©es, arr√™t du stream")
                self.stop_stream()
                
        except Exception as e:
            logger.error(f"[{self.name}] Erreur lors de la gestion de la mort du processus: {e}")

    def _check_stream_health(self) -> bool:
        """V√©rifie la sant√© du stream"""
        try:
            if not self.process_manager.is_running():
                if self.add_error("process_not_running"):
                    logger.warning(f"[{self.name}] Processus FFmpeg arr√™t√©, red√©marrage n√©cessaire")
                    self._restart_stream()
                return False

            # V√©rifier les erreurs critiques
            if self.has_critical_errors():
                logger.error(f"[{self.name}] Erreurs critiques d√©tect√©es")
                self.stop_stream()
                return False

            # V√©rifier la continuit√© du stream
            if not self._check_stream_continuity():
                if self.add_error("stream_discontinuity"):
                    logger.warning(f"[{self.name}] Discontinuit√© d√©tect√©e, red√©marrage n√©cessaire")
                    self._restart_stream()
                return False

            # Si tout est OK, reset les erreurs
            self.reset_errors()
            return True

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur check sant√©: {e}")
            return False

    def _handle_segment_created(self, segment_path, size):
        """G√®re la cr√©ation d'un nouveau segment HLS"""
        if self.logger:
            self.logger.log_segment(segment_path, size)

        # MAJ des stats de segments
        if hasattr(self, "stats_collector"):
            # Extraction de l'ID du segment depuis le nom
            segment_id = Path(segment_path).stem.split("_")[-1]
            self.stats_collector.update_segment_stats(self.name, segment_id, size)
        
        # Mise √† jour du timestamp du dernier segment
        self.last_segment_time = time.time()
        logger.debug(f"[{self.name}] ‚è±Ô∏è Timestamp de segment mis √† jour suite √† cr√©ation de {Path(segment_path).name}")

    def _scan_videos(self) -> bool:
        """Scanne les fichiers vid√©os et met √† jour processed_videos"""
        try:
            source_dir = Path(self.video_dir)
            ready_to_stream_dir = source_dir / "ready_to_stream"

            # V√©rifier d'abord le processeur
            if not self._verify_processor():
                logger.error(f"[{self.name}] ‚ùå √âchec de la v√©rification du processeur")
                self.ready_for_streaming = False
                return False

            # Cr√©ation du dossier s'il n'existe pas
            ready_to_stream_dir.mkdir(exist_ok=True)

            # V√©rifier et d√©placer les fichiers invalides dans "ready_to_stream"
            self._check_and_move_invalid_files()

            # On r√©initialise la liste des vid√©os trait√©es
            old_processed = self.processed_videos
            self.processed_videos = []

            # On scanne d'abord les vid√©os dans ready_to_stream
            mp4_files = list(ready_to_stream_dir.glob("*.mp4"))

            if not mp4_files:
                logger.warning(
                    f"[{self.name}] ‚ö†Ô∏è Aucun fichier MP4 dans {ready_to_stream_dir}"
                )

                # On v√©rifie s'il y a des fichiers √† traiter
                video_extensions = (".mp4", ".avi", ".mkv", ".mov", ".m4v")
                source_files = []
                for ext in video_extensions:
                    source_files.extend(source_dir.glob(f"*{ext}"))

                if not source_files:
                    logger.warning(
                        f"[{self.name}] ‚ö†Ô∏è Aucun fichier vid√©o dans {self.video_dir}"
                    )
                    self.ready_for_streaming = False
                    return False

                logger.info(
                    f"[{self.name}] üîÑ {len(source_files)} fichiers sources √† traiter"
                )
                self.ready_for_streaming = False
                return False

            # Log explicite des fichiers trouv√©s
            logger.info(
                f"[{self.name}] üîç {len(mp4_files)} fichiers MP4 trouv√©s dans ready_to_stream: {[f.name for f in mp4_files]}"
            )

            # V√©rification que les fichiers sont valides
            valid_files = []
            for video_file in mp4_files:
                if verify_file_ready(video_file):
                    valid_files.append(video_file)
                else:
                    logger.warning(
                        f"[{self.name}] ‚ö†Ô∏è Fichier {video_file.name} ignor√© car non valide"
                    )

            if valid_files:
                self.processed_videos.extend(valid_files)
                logger.info(
                    f"[{self.name}] ‚úÖ {len(valid_files)} vid√©os valides trouv√©es dans ready_to_stream"
                )

                # V√©rifie si la liste a chang√©
                old_names = {f.name for f in old_processed}
                new_names = {f.name for f in valid_files}

                if old_names != new_names:
                    logger.info(f"[{self.name}] üîÑ Liste des vid√©os modifi√©e:")
                    if old_names - new_names:
                        logger.info(f"   - Supprim√©es: {old_names - new_names}")
                    if new_names - old_names:
                        logger.info(f"   + Ajout√©es: {new_names - old_names}")

                    # Mise √† jour de la playlist
                    threading.Thread(
                        target=self._create_concat_file, daemon=True
                    ).start()

                # La cha√Æne est pr√™te si on a des vid√©os valides
                self.ready_for_streaming = True

                # Notifier le manager que cette cha√Æne est pr√™te
                self._notify_manager_ready()

                return True
            else:
                logger.warning(
                    f"[{self.name}] ‚ö†Ô∏è Aucun fichier MP4 valide trouv√© dans ready_to_stream"
                )
                self.ready_for_streaming = False
                return False

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur scan des vid√©os: {str(e)}")
            import traceback

            logger.error(f"[{self.name}] {traceback.format_exc()}")
            return False

    def _notify_manager_ready(self):
        """Notifie le manager que cette cha√Æne est pr√™te pour le streaming"""
        try:
            # On cherche le manager dans le registre global des cha√Ænes
            if hasattr(FFmpegProcessManager, "all_channels"):
                for name, channel in FFmpegProcessManager.all_channels.items():
                    if name == self.name:
                        # On met √† jour le statut dans le manager
                        if hasattr(channel, "manager") and channel.manager:
                            with channel.manager.scan_lock:
                                channel.manager.channel_ready_status[self.name] = True
                            logger.info(f"[{self.name}] ‚úÖ Statut 'pr√™t' mis √† jour dans le manager")
                        break
        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur notification manager: {e}")

    def start_stream(self) -> bool:
        """
        D√©marre le flux HLS pour cette cha√Æne via FFmpeg en mode s√©quentiel.
        """
        try:
            # 1) V√©rifier qu'on a des vid√©os pr√™tes
            if not self.ready_for_streaming:
                logger.warning(
                    f"[{self.name}] ‚ö†Ô∏è Cha√Æne non pr√™te (pas de vid√©os). Annulation du d√©marrage."
                )
                return False

            # 2) Nettoyer le dossier HLS (playlist/segments) avant de lancer FFmpeg
            hls_dir = Path(f"/app/hls/{self.name}")
            if hls_dir.exists():
                # Supprime d'abord les segments existants
                for seg in hls_dir.glob("*.ts"):
                    try:
                        seg.unlink()
                    except Exception as e:
                        logger.warning(
                            f"[{self.name}] Erreur suppression segment {seg.name}: {e}"
                        )

                # Supprime l'ancienne playlist
                old_playlist = hls_dir / "playlist.m3u8"
                if old_playlist.exists():
                    try:
                        old_playlist.unlink()
                    except Exception as e:
                        logger.warning(
                            f"[{self.name}] Erreur suppression playlist: {e}"
                        )
            else:
                hls_dir.mkdir(parents=True, exist_ok=True)

            # 3) D√©marrer le thread de streaming s√©quentiel
            if not hasattr(self, "_streaming_thread") or not self._streaming_thread.is_alive():
                self._streaming_thread = threading.Thread(
                    target=self._stream_files_sequentially,
                    daemon=True
                )
                self._streaming_thread.start()
                logger.info(f"[{self.name}] üîÑ Thread de streaming s√©quentiel d√©marr√©")

            # 4) D√©marrer la boucle de surveillance des segments en arri√®re-plan
            if not hasattr(self, "_segment_monitor_thread") or not self._segment_monitor_thread.is_alive():
                self._segment_monitor_thread = threading.Thread(
                    target=self._segment_monitor_loop,
                    daemon=True
                )
                self._segment_monitor_thread.start()
                logger.info(f"[{self.name}] üîç Boucle de surveillance des segments d√©marr√©e")

            logger.info(f"[{self.name}] ‚úÖ Stream d√©marr√© avec succ√®s.")
            return True

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur start_stream: {e}")
            return False

    def _stream_files_sequentially(self):
        """
        Stream les fichiers MP4 un par un s√©quentiellement depuis le dossier ready_to_stream.
        Cette m√©thode s'ex√©cute dans un thread d√©di√©.
        """
        try:
            self._streaming_active = True
            error_count = 0
            max_errors = 3  # Nombre maximum d'erreurs cons√©cutives
            
            while self._streaming_active:
                try:
                    # V√©rifier le dossier ready_to_stream
                    ready_dir = Path(self.video_dir) / "ready_to_stream"
                    if not ready_dir.exists():
                        logger.error(f"[{self.name}] ‚ùå Dossier ready_to_stream introuvable")
                        time.sleep(1)
                        continue
                    
                    # R√©cup√©rer et v√©rifier les fichiers MP4
                    video_files = list(ready_dir.glob("*.mp4"))
                    if not video_files:
                        logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 dans ready_to_stream")
                        time.sleep(1)
                        continue
                    
                    # V√©rifier que les fichiers sont valides
                    valid_files = []
                    for video_file in video_files:
                        if verify_file_ready(video_file):
                            valid_files.append(video_file)
                        else:
                            logger.warning(f"[{self.name}] ‚ö†Ô∏è Fichier {video_file.name} ignor√© car non valide")
                    
                    if not valid_files:
                        logger.error(f"[{self.name}] ‚ùå Aucun fichier MP4 valide trouv√©")
                        time.sleep(1)
                        continue
                    
                    # M√©langer les fichiers pour plus de vari√©t√©
                    import random
                    random.shuffle(valid_files)
                    
                    logger.info(f"[{self.name}] üé¨ D√©but du streaming s√©quentiel avec {len(valid_files)} fichiers")
                    
                    # Streamer chaque fichier s√©quentiellement
                    for video_file in valid_files:
                        if not self._streaming_active:
                            logger.info(f"[{self.name}] ‚èπÔ∏è Arr√™t du streaming s√©quentiel demand√©")
                            return
                            
                        retry_count = 0
                        max_retries = 2
                        while retry_count < max_retries:
                            logger.info(f"[{self.name}] üé¨ D√©but du streaming de {video_file.name} (tentative {retry_count + 1}/{max_retries})")
                            success = self._stream_single_file(video_file)
                            
                            if success:
                                error_count = 0  # R√©initialiser le compteur d'erreurs en cas de succ√®s
                                logger.info(f"[{self.name}] ‚úÖ Streaming de {video_file.name} termin√© avec succ√®s")
                                break
                            
                            retry_count += 1
                            if retry_count < max_retries:
                                logger.warning(f"[{self.name}] ‚ö†Ô∏è √âchec du streaming, nouvelle tentative dans 1s...")
                                time.sleep(1)
                        
                        if not success and self._streaming_active:
                            logger.error(f"[{self.name}] ‚ùå √âchec du streaming de {video_file.name} apr√®s {max_retries} tentatives")
                            error_count += 1
                            if error_count >= max_errors:
                                logger.warning(f"[{self.name}] ‚ö†Ô∏è Trop d'erreurs ({error_count}), red√©marrage du streaming")
                                return self._restart_stream()
                    
                    # Si on arrive ici, on a fini la liste des fichiers, on recommence
                    logger.info(f"[{self.name}] üîÑ Fin de la liste des fichiers, red√©marrage")
                    
                except Exception as e:
                    logger.error(f"[{self.name}] ‚ùå Erreur dans la boucle de streaming: {e}")
                    error_count += 1
                    if error_count >= max_errors:
                        logger.warning(f"[{self.name}] ‚ö†Ô∏è Trop d'erreurs ({error_count}), red√©marrage du streaming")
                        return self._restart_stream()
                    time.sleep(1)
                    
        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur fatale dans le streaming s√©quentiel: {e}")
        finally:
            self._streaming_active = False
            logger.info(f"[{self.name}] ‚èπÔ∏è Fin du streaming s√©quentiel")
    
    def _stream_single_file(self, file_path):
        """Stream un fichier unique avec ffmpeg"""
        try:
            hls_dir = Path(f"/app/hls/{self.name}")
            
            # Obtenir les fichiers de logs
            ffmpeg_log = Path(f"/app/logs/ffmpeg/{self.name}_ffmpeg.log")
            progress_log = Path(f"/app/logs/ffmpeg/{self.name}_progress.log")
            
            # S'assurer que les dossiers de logs existent
            ffmpeg_log.parent.mkdir(parents=True, exist_ok=True)
            progress_log.parent.mkdir(parents=True, exist_ok=True)

            # V√©rifier la dur√©e du fichier avant de commencer
            duration = get_accurate_duration(file_path)
            if not duration or duration <= 0:
                logger.error(f"[{self.name}] ‚ùå Dur√©e invalide pour {file_path}")
                return False

            # Utiliser le FFmpegCommandBuilder pour construire la commande
            command = self.command_builder.build_command(
                input_file=file_path,
                output_dir=str(hls_dir),
                progress_file=str(progress_log)
            )
            
            # Log de la commande compl√®te
            logger.info("=" * 80)
            logger.info(f"[{self.name}] üöÄ D√©marrage streaming: {os.path.basename(file_path)}")
            logger.info(f"$ {' '.join(command)}")
            logger.info("=" * 80)
            
            # Variables pour le monitoring
            start_time = time.time()
            last_progress_time = start_time
            last_segment_count = 0
            stall_detected = False
            stall_start_time = None
            
            # D√©marrer le processus avec redirection des logs
            with open(ffmpeg_log, "a") as log_file:
                self._current_process = subprocess.Popen(
                    command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    universal_newlines=True,
                    bufsize=1
                )
                
                # Boucle de lecture des logs en temps r√©el avec monitoring
                while self._current_process.poll() is None:
                    current_time = time.time()
                    
                    # V√©rifier le timeout global (5 minutes)
                    if current_time - start_time > 300:
                        logger.error(f"[{self.name}] ‚ùå Timeout global atteint (5 minutes)")
                        self._current_process.terminate()
                        return False
                    
                    # Lire stderr pour les logs FFmpeg
                    stderr_line = self._current_process.stderr.readline()
                    if stderr_line:
                        log_file.write(stderr_line)
                        log_file.flush()
                        
                        # Log les erreurs importantes
                        if "error" in stderr_line.lower():
                            logger.error(f"[{self.name}] FFmpeg: {stderr_line.strip()}")
                        elif "warning" in stderr_line.lower():
                            logger.warning(f"[{self.name}] FFmpeg: {stderr_line.strip()}")
                    
                    # V√©rifier la progression toutes les 5 secondes
                    if current_time - last_progress_time >= 5:
                        # Compter les segments actuels
                        current_segments = len(list(hls_dir.glob("segment_*.ts")))
                        
                        # D√©tecter un stall si pas de nouveaux segments
                        if current_segments == last_segment_count:
                            # Ne consid√©rer comme stall que si on a d√©j√† g√©n√©r√© un nombre raisonnable de segments
                            # et √©viter les faux positifs au d√©but du streaming
                            if current_segments >= 3:
                                if not stall_detected:
                                    stall_detected = True
                                    stall_start_time = current_time
                                    # Utiliser debug au lieu de warning pour r√©duire le bruit dans les logs
                                    logger.debug(f"[{self.name}] ‚ÑπÔ∏è Stall potentiel dans la g√©n√©ration des segments")
                                
                                # Augmenter le d√©lai de stall de 30s √† 60s pour √™tre moins strict
                                if current_time - stall_start_time > 60:
                                    logger.error(f"[{self.name}] ‚ùå Stall prolong√© d√©tect√©, red√©marrage")
                                    self._current_process.terminate()
                                    return False
                            else:
                                # Pour les streams qui d√©marrent, √™tre plus patient
                                if not stall_detected:
                                    stall_detected = True
                                    stall_start_time = current_time
                                # Utiliser un d√©lai encore plus long pour les d√©buts de stream (120s)
                                elif current_time - stall_start_time > 120:
                                    logger.warning(f"[{self.name}] ‚ö†Ô∏è D√©marrage lent, segments insuffisants apr√®s 2 minutes")
                                    self._current_process.terminate()
                                    return False
                        else:
                            stall_detected = False
                            stall_start_time = None
                        
                        last_segment_count = current_segments
                        last_progress_time = current_time
                        
                        # Log de progression
                        logger.debug(f"[{self.name}] üìä Progression: {current_segments} segments g√©n√©r√©s")
                
                # V√©rifier le code de retour
                return_code = self._current_process.poll() or 0
                if return_code != 0:
                    logger.error(f"[{self.name}] ‚ùå Erreur FFmpeg ({return_code})")
                    return False
                
                # V√©rifier que des segments ont √©t√© g√©n√©r√©s
                final_segments = len(list(hls_dir.glob("segment_*.ts")))
                if final_segments == 0:
                    logger.error(f"[{self.name}] ‚ùå Aucun segment g√©n√©r√©")
                    return False
                
                logger.info(f"[{self.name}] ‚úÖ Streaming termin√© avec succ√®s ({final_segments} segments)")
                return True

        except Exception as e:
            logger.error(f"[{self.name}] ‚ùå Erreur streaming {file_path}: {e}")
            return False

    def _should_stop_stream(self):
        """V√©rifie si le stream doit √™tre arr√™t√©"""
        # Ne pas arr√™ter le stream s'il n'y a pas d'erreurs, m√™me sans watchers
        if not self.error_handler.has_critical_errors():
            return False
        
        # Arr√™ter uniquement en cas d'erreurs critiques
        return True

    def check_watchers_timeout(self):
        """V√©rifie si le stream doit √™tre arr√™t√© en raison d'une absence de watchers"""
        # On ne v√©rifie pas le timeout s'il n'y a pas de watchers_count
        if not hasattr(self, "watchers_count"):
            return False
            
        # On ne v√©rifie pas le timeout si le stream n'est pas actif
        if not self.process_manager.is_running():
            return False
            
        # On ne v√©rifie pas le timeout s'il y a des watchers actifs
        if self.watchers_count > 0:
            return False
            
        # On ne v√©rifie pas le timeout s'il y a des erreurs critiques
        if self.error_handler.has_critical_errors():
            return False
            
        # Le stream continue de tourner m√™me sans watchers
        return False

    def has_critical_errors(self) -> bool:
        """V√©rifie si des erreurs critiques sont pr√©sentes"""
        # On consid√®re qu'il y a une erreur critique si :
        # - On a eu trop de red√©marrages
        if self.restart_count >= self.max_restarts:
            return True
            
        # - On a eu trop d'erreurs du m√™me type
        if self.error_count >= 3 and len(self.error_types) == 1:
            return True
            
        # - On a eu plusieurs types d'erreurs diff√©rentes
        if len(self.error_types) >= 3:
            return True
            
        return False

    def add_error(self, error_type: str) -> bool:
        """Ajoute une erreur et retourne True si un restart est n√©cessaire"""
        self.error_count += 1
        self.error_types.add(error_type)
        
        logger.warning(f"[{self.name}] Erreur d√©tect√©e: {error_type}, total: {self.error_count}")
        
        # Log le crash
        self._log_crash(error_type)

        # On regroupe les erreurs similaires
        if self.error_count >= 3 and len(self.error_types) >= 2:
            return self.should_restart()
        return False

    def should_restart(self) -> bool:
        """V√©rifie si on peut/doit red√©marrer"""
        if self.restart_count >= self.max_restarts:
            logger.error(f"[{self.name}] Trop de red√©marrages")
            return False
            
        current_time = time.time()
        if current_time - self.last_restart_time < self.restart_cooldown:
            return False
            
        self.restart_count += 1
        self.last_restart_time = current_time
        
        # Log le red√©marrage
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        try:
            with open(self.crash_log_path, "a") as f:
                f.write(f"{timestamp} - [{self.name}] Red√©marrage #{self.restart_count}\n")
                f.write("-" * 80 + "\n")
        except Exception as e:
            logger.error(f"Erreur lors de l'√©criture du log de red√©marrage: {e}")
        
        self.error_count = 0
        self.error_types.clear()
        return True

    def reset_errors(self):
        """Reset apr√®s un stream stable"""
        if self.error_count > 0 or self.restart_count > 0:
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            try:
                with open(self.crash_log_path, "a") as f:
                    f.write(f"{timestamp} - [{self.name}] Reset des erreurs\n")
                    f.write("-" * 80 + "\n")
            except Exception as e:
                logger.error(f"Erreur lors de l'√©criture du log de reset: {e}")
                
        self.error_count = 0
        self.restart_count = 0
        self.error_types.clear()
        return True

    def _log_crash(self, error_type: str):
        """Log des erreurs dans un fichier de crash d√©di√©"""
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        try:
            with open(self.crash_log_path, "a") as f:
                f.write(f"{timestamp} - Erreur d√©tect√©e: {error_type}\n")
                f.write(f"Compteur d'erreurs: {self.error_count}, Types d'erreurs: {', '.join(self.error_types)}\n")
                f.write("-" * 80 + "\n")
        except Exception as e:
            logger.error(f"Erreur √©criture log crash pour {self.name}: {e}")
