# stats_collector.py
import os
import json
import time
import threading
import re
from pathlib import Path
from config import logger

class StatsCollector:
    """
    # Collecte et sauvegarde des statistiques de lecture des cha√Ænes
    # Permet l'analyse future des habitudes de visionnage
    """

    # Dans stats_collector.py

    def __init__(self, stats_dir="/app/stats"):
        """Initialise le collecteur de statistiques"""
        self.stats_dir = Path(stats_dir)
        self.stats_dir.mkdir(parents=True, exist_ok=True)

        self.stats_file = self.stats_dir / "channel_stats.json"
        self.user_stats_file = self.stats_dir / "user_stats.json"

        # Initialisation des structures de donn√©es
        self.stats = {}  # {channel: {total_watch_time, unique_viewers, watchlist, last_update}}
        self.global_stats = {
            "total_watch_time": 0.0,
            "unique_viewers": set(),
            "last_update": time.time()
        }
        self.user_stats = {}  # {ip: {total_watch_time, channels_watched, last_seen, user_agent}}
        self.daily_stats = {}  # {date: {channel: {total_watch_time, unique_viewers}}}
        self.last_save_time = time.time()
        self.lock = threading.Lock()
        self.save_interval = 300  # 5 minutes

        # Chargement des stats existantes
        self.stats, self.global_stats = self._load_stats()
        self.user_stats = self._load_user_stats()

        # D√©marrage du thread de sauvegarde p√©riodique
        self.stop_save_thread = threading.Event()
        self.save_thread = threading.Thread(target=self._save_loop, daemon=True)
        self.save_thread.start()

        # Forcer une sauvegarde initiale des deux fichiers
        # self.save_stats()  # Remove initial save
        # self.save_user_stats() # Remove initial save

        logger.info(
            f"üìä StatsCollector initialis√© (sauvegarde dans {self.stats_file}, user stats dans {self.user_stats_file})"
        )

        # Ajout des m√©triques de performance
        self.buffer_issues = {}  # Dictionnaire pour stocker les probl√®mes de buffer par cha√Æne
        self.latency_issues = {}  # Dictionnaire pour stocker les probl√®mes de latence par cha√Æne
        self.performance_metrics = {}  # Dictionnaire pour stocker les m√©triques de performance par cha√Æne

    def add_watch_time(self, channel, ip, duration):
        """Ajoute du temps de visionnage pour un watcher avec limitation de fr√©quence"""
        with self.lock:
            try:
                current_time = time.time()
                
                # V√©rifie la derni√®re mise √† jour pour cette paire channel/ip
                update_key = f"{channel}:{ip}"
                if not hasattr(self, "_last_update_times"):
                    self._last_update_times = {}
                
                # Limite les mises √† jour √† une fois par seconde maximum
                if update_key in self._last_update_times:
                    last_update = self._last_update_times[update_key]
                    elapsed = current_time - last_update
                    
                    # Si moins d'une seconde depuis la derni√®re mise √† jour, ajuster la dur√©e
                    if elapsed < 1.0:
                        # On ignore cette mise √† jour trop rapproch√©e
                        logger.debug(f"[STATS] Mise √† jour trop rapide pour {channel}:{ip} (interval: {elapsed:.2f}s), ignor√©e")
                        return
                    
                    # Ajuster la dur√©e en fonction du temps r√©el √©coul√©
                    if elapsed < duration and duration > 2.0:
                        adjusted_duration = max(elapsed, 1.0)  # Au moins 1 seconde
                        logger.debug(f"[STATS] Dur√©e ajust√©e pour {channel}:{ip}: {duration:.1f}s ‚Üí {adjusted_duration:.1f}s")
                        duration = adjusted_duration
                
                # Enregistrer le moment de cette mise √† jour
                self._last_update_times[update_key] = current_time
                
                # Initialisation des stats si n√©cessaire
                if channel not in self.stats:
                    self.stats[channel] = {
                        "total_watch_time": 0.0,
                        "unique_viewers": set(),
                        "watchlist": {},  # {ip: total_time}
                        "last_update": time.time()
                    }

                # Mise √† jour des stats du canal
                channel_stats = self.stats[channel]
                channel_stats["total_watch_time"] += duration
                channel_stats["unique_viewers"].add(ip)
                channel_stats["last_update"] = time.time()

                # Initialiser watchlist si elle n'existe pas
                if "watchlist" not in channel_stats:
                    channel_stats["watchlist"] = {}

                # Mise √† jour de la watchlist pour cette IP
                if ip not in channel_stats["watchlist"]:
                    channel_stats["watchlist"][ip] = 0.0
                    logger.debug(f"[STATS] üìä Nouvelle IP ajout√©e √† la watchlist: {ip} sur {channel}")
                channel_stats["watchlist"][ip] += duration

                # Mise √† jour des stats globales
                # Removed the block updating self.stats["global"] as it's redundant
                # Global stats are now calculated correctly in save_stats from self.global_stats

                # Mise √† jour des stats utilisateur (avec v√©rification des champs manquants)
                if ip not in self.user_stats:
                    # Structure compl√®te pour nouvel utilisateur
                    self.user_stats[ip] = {
                        "total_watch_time": 0.0,
                        "channels_watched": set(),
                        "last_seen": time.time(),
                        "user_agent": None,
                        "channels": {}
                    }
                
                user = self.user_stats[ip]
                user["total_watch_time"] = user.get("total_watch_time", 0.0) + duration
                
                # V√©rifier si channels_watched existe et l'initialiser si n√©cessaire
                if "channels_watched" in user:
                    # Convert list to set if needed
                    if isinstance(user["channels_watched"], list):
                        user["channels_watched"] = set(user["channels_watched"])
                else:
                    user["channels_watched"] = set()
                user["channels_watched"].add(channel)
                
                user["last_seen"] = time.time()

                # S'assurer que channels existe
                if "channels" not in user:
                    user["channels"] = {}

                # Init pour cette cha√Æne si n√©cessaire
                if channel not in user["channels"]:
                    user["channels"][channel] = {
                        "first_seen": int(time.time()),
                        "last_seen": int(time.time()),
                        "total_watch_time": 0,
                        "favorite": False,
                    }

                # MAJ des stats de la cha√Æne
                channel_data = user["channels"][channel]
                channel_data["last_seen"] = int(time.time())
                channel_data["total_watch_time"] += duration

                # D√©termination de la cha√Æne favorite
                if len(user["channels"]) > 1:
                    favorite_channel = max(
                        user["channels"].items(), key=lambda x: x[1]["total_watch_time"]
                    )[0]

                    for ch_name, ch_data in user["channels"].items():
                        ch_data["favorite"] = ch_name == favorite_channel

                # Mise √† jour des stats quotidiennes
                self._update_daily_stats(channel, ip, duration)

            except Exception as e:
                logger.error(f"‚ùå Erreur mise √† jour stats: {e}")
                import traceback
                logger.error(traceback.format_exc())
            
    def _save_loop(self):
        """Sauvegarde p√©riodique des statistiques"""
        while not self.stop_save_thread.is_set():
            try:
                # Attente entre sauvegardes
                time.sleep(self.save_interval)

                # Sauvegarde des stats principales
                self.save_stats()

                # Sauvegarde des stats utilisateur
                self.save_user_stats()

            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde p√©riodique des stats: {e}")

    def _load_user_stats(self):
        """Charge les stats utilisateurs ou cr√©e un nouveau fichier"""
        if self.user_stats_file.exists():
            try:
                with open(self.user_stats_file, "r") as f:
                    loaded_data = json.load(f)
                    
                    # Cr√©ation d'une structure vide
                    clean_stats = {
                        "last_updated": int(time.time()),
                    }
                    
                    # Extraction des donn√©es utilisateurs depuis n'importe quel niveau d'imbrication
                    def extract_users(data_obj):
                        if not isinstance(data_obj, dict):
                            return {}
                        
                        extracted = {}
                        
                        # Si l'objet contient une cl√© "users" qui est un dictionnaire
                        if "users" in data_obj and isinstance(data_obj["users"], dict):
                            # Parcourir les utilisateurs de premier niveau
                            for ip, user_data in data_obj["users"].items():
                                if ip != "users" and isinstance(user_data, dict):
                                    # C'est un vrai utilisateur
                                    extracted[ip] = user_data
                            
                            # R√©cursion pour extraire les utilisateurs des niveaux imbriqu√©s
                            nested_users = extract_users(data_obj["users"])
                            # Fusionner avec les utilisateurs d√©j√† extraits
                            for ip, user_data in nested_users.items():
                                extracted[ip] = user_data
                                
                        return extracted
                    
                    # Extraire tous les utilisateurs et les ajouter √† clean_stats
                    users = extract_users(loaded_data)
                    for ip, user_data in users.items():
                        clean_stats[ip] = user_data
                    
                    return clean_stats
                    
            except json.JSONDecodeError:
                logger.warning(f"‚ö†Ô∏è Fichier stats utilisateurs corrompu, cr√©ation d'un nouveau")

        # Structure initiale
        return {
            "last_updated": int(time.time()),
        }
    
    def save_user_stats(self):
        """Sauvegarde les statistiques par utilisateur"""
        with self.lock:
            try:
                # V√©rifications de base
                if not hasattr(self, "user_stats") or not self.user_stats:
                    self.user_stats = {"users": {}, "last_updated": int(time.time())}

                # S'assurer que le dossier existe
                os.makedirs(os.path.dirname(self.user_stats_file), exist_ok=True)

                # Pr√©paration des donn√©es s√©rialisables - STRUCTURE SIMPLIFI√âE SANS IMBRICATION
                serializable_stats = {
                    "users": {},
                    "last_updated": int(time.time())
                }
                
                # Parcourir le dictionnaire user_stats pour extraire uniquement les donn√©es utilisateurs
                for ip, user_data in self.user_stats.items():
                    # Ignorer les cl√©s de m√©tadonn√©es comme "last_updated"
                    if ip == "last_updated":
                        continue
                        
                    # Convertir les donn√©es utilisateur en format s√©rialisable
                    serializable_user = {}
                    for key, val in user_data.items():
                        # Convertir les sets en listes
                        if isinstance(val, set):
                            serializable_user[key] = list(val)
                        else:
                            serializable_user[key] = val
                    
                    # Ajouter cet utilisateur au dictionnaire principal
                    serializable_stats["users"][ip] = serializable_user

                # Sauvegarde effective
                with open(self.user_stats_file, "w") as f:
                    json.dump(serializable_stats, f, indent=2)

                logger.debug(f"‚úÖ Stats utilisateurs sauvegard√©es dans {self.user_stats_file}")
                return True
            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde stats utilisateurs: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
    
    def update_user_stats(self, ip, channel_name, duration, user_agent=None):
        """Met √† jour les stats par utilisateur"""
        with self.lock:
            # Log initial
            logger.debug(f"üë§ Mise √† jour stats utilisateur: {ip} sur {channel_name} (+{duration:.1f}s)")

            # S'assurer que user_stats et users existent
            if not hasattr(self, "user_stats"):
                logger.warning("‚ö†Ô∏è Initialisation de user_stats")
                self.user_stats = {"users": {}, "last_updated": int(time.time())}

            if "users" not in self.user_stats:
                self.user_stats["users"] = {}

            # Init pour cet utilisateur si n√©cessaire
            if ip not in self.user_stats["users"]:
                self.user_stats["users"][ip] = {
                    "first_seen": int(time.time()),
                    "last_seen": int(time.time()),
                    "total_watch_time": 0,
                    "channels": {},
                    "user_agent": user_agent,
                }
                logger.info(f"üë§ Nouvel utilisateur d√©tect√©: {ip}")

            user = self.user_stats["users"][ip]
            old_total_time = user["total_watch_time"]
            user["last_seen"] = int(time.time())
            user["total_watch_time"] += duration

            # Log de la mise √† jour du temps total
            logger.info(
                f"‚è±Ô∏è {ip}: temps total pass√© de {old_total_time:.1f}s √† {user['total_watch_time']:.1f}s"
            )

            # MAJ de l'user agent si fourni
            if user_agent:
                user["user_agent"] = user_agent

            # S'assurer que channels existe
            if "channels" not in user:
                user["channels"] = {}

            # Init pour cette cha√Æne si n√©cessaire
            if channel_name not in user["channels"]:
                user["channels"][channel_name] = {
                    "first_seen": int(time.time()),
                    "last_seen": int(time.time()),
                    "total_watch_time": 0,
                    "favorite": False,
                }
                logger.info(f"üì∫ {ip}: nouvelle cha√Æne {channel_name}")

            # MAJ des stats de la cha√Æne
            channel = user["channels"][channel_name]
            old_channel_time = channel["total_watch_time"]
            channel["last_seen"] = int(time.time())
            channel["total_watch_time"] += duration

            # Log de la mise √† jour du temps par cha√Æne
            logger.info(
                f"üì∫ {ip} sur {channel_name}: temps pass√© de {old_channel_time:.1f}s √† {channel['total_watch_time']:.1f}s"
            )

            # D√©termination de la cha√Æne favorite
            if len(user["channels"]) > 1:
                favorite_channel = max(
                    user["channels"].items(), key=lambda x: x[1]["total_watch_time"]
                )[0]

                for ch_name, ch_data in user["channels"].items():
                    ch_data["favorite"] = ch_name == favorite_channel

            # Sauvegarde p√©riodique
            # Removed conditional save block:
            # if (
            #     not hasattr(self, "last_user_save")
            #     or time.time() - self.last_user_save > 300
            # ):
            #     threading.Thread(target=self.save_user_stats, daemon=True).start()
            #     self.last_user_save = time.time()
            #     logger.info("üíæ Sauvegarde forc√©e des stats utilisateur apr√®s 5 minutes d'activit√©")

    def _load_stats(self):
        """Charge les stats existantes ou cr√©e un nouveau fichier"""
        if self.stats_file.exists():
            try:
                with open(self.stats_file, "r") as f:
                    loaded_stats = json.load(f)
                    
                    # Conversion de la structure charg√©e vers la nouvelle structure
                    stats = {}
                    # Initialize global_stats with defaults, to be overwritten if found in file
                    global_stats = {
                        "total_watch_time": 0.0,
                        "unique_viewers": set(),
                        "last_update": time.time()
                    }
                    
                    # Conversion des stats des cha√Ænes
                    if "channels" in loaded_stats:
                        for channel_name, channel_data in loaded_stats["channels"].items():
                            stats[channel_name] = {
                                "total_watch_time": float(channel_data.get("total_watch_time", 0)),
                                "unique_viewers": set(channel_data.get("unique_viewers", [])),
                                "watchlist": channel_data.get("watchlist", {}),
                                "last_update": time.time()
                            }
                    
                    # Conversion des stats globales
                    if "global" in loaded_stats:
                        global_data = loaded_stats["global"]
                        global_stats["total_watch_time"] = float(global_data.get("total_watch_time", 0))
                        global_stats["unique_viewers"] = set(global_data.get("unique_viewers", []))
                        global_stats["last_update"] = time.time()
                    
                    # Mise √† jour des attributs - REMOVED, assignment happens in __init__
                    # self.stats = stats
                    # self.global_stats = global_stats
                    
                    logger.debug(f"üìä Stats charg√©es: {len(stats)} cha√Ænes, {len(global_stats['unique_viewers'])} spectateurs uniques")
                    return stats, global_stats # Modified: Return both
                    
            except json.JSONDecodeError:
                logger.warning(f"‚ö†Ô∏è Fichier de stats corrompu, cr√©ation d'un nouveau")

        # Structure initiale des stats en cas d'√©chec du chargement ou fichier inexistant
        default_global_stats = {
            "total_watch_time": 0.0,
            "unique_viewers": set(),
            "last_update": time.time()
        }
        return {}, default_global_stats # Modified: Return default structures for both

    def save_stats(self):
        """Sauvegarde les statistiques dans le fichier JSON"""
        with self.lock:
            try:
                # Calculate global statistics from all channels in self.stats
                total_watch_time = 0.0
                all_viewers = set()
                
                for channel_name, channel_data in self.stats.items():
                    # Removed check for channel_name == "global"
                    total_watch_time += channel_data.get("total_watch_time", 0)
                    all_viewers.update(channel_data.get("unique_viewers", set()))
                
                # Update self.global_stats before saving
                self.global_stats["total_watch_time"] = total_watch_time
                self.global_stats["unique_viewers"] = all_viewers
                self.global_stats["last_update"] = time.time()
                
                # Pr√©paration des donn√©es √† sauvegarder
                stats_to_save = {
                    "channels": {},
                    "global": {
                        "total_watch_time": self.global_stats["total_watch_time"],
                        "unique_viewers": list(self.global_stats["unique_viewers"]),
                        "last_update": self.global_stats["last_update"]
                    }
                }

                # Conversion des stats des cha√Ænes
                for channel_name, channel_data in self.stats.items():
                    # Removed check for channel_name == "global"
                        
                    # Ensure all required keys exist
                    channel_stats = {
                        "total_watch_time": channel_data.get("total_watch_time", 0),
                        "unique_viewers": list(channel_data.get("unique_viewers", set())),
                        "last_update": channel_data.get("last_update", time.time())
                    }
                    
                    # Add watchlist only if it exists
                    if "watchlist" in channel_data:
                        channel_stats["watchlist"] = channel_data["watchlist"]

                    stats_to_save["channels"][channel_name] = channel_stats

                # Sauvegarde dans le fichier principal
                with open(self.stats_file, "w") as f:
                    json.dump(stats_to_save, f, indent=2)

                # Sauvegarde d'une copie horodat√©e toutes les 24h
                now = time.time()
                last_daily = getattr(self, "last_daily_save", 0)
                if now - last_daily > 86400:  # 24h
                    date_str = time.strftime("%Y-%m-%d")
                    daily_file = self.stats_dir / f"channel_stats_{date_str}.json"
                    with open(daily_file, "w") as f:
                        json.dump(stats_to_save, f, indent=2)
                    self.last_daily_save = now
                    logger.info(f"üìä Sauvegarde quotidienne des stats: {daily_file}")

                logger.debug(
                    f"üìä Statistiques sauvegard√©es ({len(self.stats)} cha√Ænes, {len(self.global_stats['unique_viewers'])} spectateurs uniques)"
                )
                return True
            except Exception as e:
                logger.error(f"‚ùå Erreur sauvegarde stats: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
    
    def update_channel_watchers(self, channel_name, watchers_count):
        """Met √† jour les stats de watchers pour une cha√Æne"""
        with self.lock:
            # Init des stats pour cette cha√Æne si n√©cessaire
            if channel_name not in self.stats:
                logger.warning(f"Attempting to update watchers for non-existent channel {channel_name}. Skipping.")
                return

            # Access channel stats directly
            channel_stats = self.stats[channel_name]

            # Ensure necessary keys exist
            if "current_watchers" not in channel_stats: channel_stats["current_watchers"] = 0
            if "peak_watchers" not in channel_stats: channel_stats["peak_watchers"] = 0
            if "peak_time" not in channel_stats: channel_stats["peak_time"] = 0
            if "session_count" not in channel_stats: channel_stats["session_count"] = 0

            old_watchers = channel_stats["current_watchers"]
            channel_stats["current_watchers"] = watchers_count

            # Mise √† jour du pic si n√©cessaire
            if watchers_count > channel_stats["peak_watchers"]:
                channel_stats["peak_watchers"] = watchers_count
                channel_stats["peak_time"] = int(time.time())

            # Si le nombre de watchers augmente, c'est une nouvelle session
            if watchers_count > old_watchers:
                channel_stats["session_count"] += watchers_count - old_watchers

            # Mise √† jour des stats globales
            # This section recalculates global watchers based on 'current_watchers'
            # But 'current_watchers' isn't reliably maintained elsewhere.
            # It's better to calculate global watchers based on active connections/sessions if needed.
            # For now, commenting out the potentially inaccurate global update.

            # total_current_watchers = sum(
            #     ch.get("current_watchers", 0) for ch_name, ch in self.stats.items()
            # )
            # # Ensure global stats exist
            # if "global" not in self.stats: # This check might be wrong, global stats are separate
            #     # self.stats["global"] = { ... } # Initialize if needed, but global stats are in self.global_stats
            #     logger.warning("Accessing self.stats['global'] which might not be intended structure.")

            # # Access global stats correctly (assuming self.global_stats is the right place)
            # if not hasattr(self, 'global_stats'): self.global_stats = {} # Initialize if missing
            # if "total_watchers" not in self.global_stats: self.global_stats["total_watchers"] = 0
            # if "peak_watchers" not in self.global_stats: self.global_stats["peak_watchers"] = 0
            # if "peak_time" not in self.global_stats: self.global_stats["peak_time"] = 0

            # self.global_stats["total_watchers"] = total_current_watchers

            # # Mise √† jour du pic global si n√©cessaire
            # if (
            #     self.global_stats["total_watchers"]
            #     > self.global_stats["peak_watchers"]
            # ):
            #     self.global_stats["peak_watchers"] = self.global_stats[
            #         "total_watchers"
            #     ]
            #     self.global_stats["peak_time"] = int(time.time())

            # Mise √† jour des stats quotidiennes
            # This also relies on potentially inaccurate global/current watchers.
            # Daily stats should ideally aggregate watch time and unique viewers from add_watch_time.
            # Commenting out for now.

            # today = time.strftime("%Y-%m-%d")
            # # Ensure daily stats structure exists
            # if not hasattr(self, 'daily_stats'): self.daily_stats = {}
            # if today not in self.daily_stats:
            #     self.daily_stats[today] = {
            #         "peak_watchers": 0,
            #         "total_watch_time": 0, # This should be aggregated from watch time updates
            #         "channels": {},
            #     }

            # daily_stats = self.daily_stats[today]

            # # Mise √† jour du pic quotidien (using potentially inaccurate global watchers)
            # # if self.global_stats.get("total_watchers", 0) > daily_stats.get("peak_watchers", 0):
            # #     daily_stats["peak_watchers"] = self.global_stats["total_watchers"]

            # # Init des stats de la cha√Æne pour aujourd'hui
            # if channel_name not in daily_stats["channels"]:
            #     daily_stats["channels"][channel_name] = {
            #         "peak_watchers": 0,
            #         "total_watch_time": 0, # Should aggregate from watch time
            #     }

            # # Mise √† jour du pic de la cha√Æne pour aujourd'hui (using potentially inaccurate watchers_count)
            # daily_channel = daily_stats["channels"][channel_name]
            # if watchers_count > daily_channel.get("peak_watchers", 0):
            #     daily_channel["peak_watchers"] = watchers_count

    def update_segment_stats(self, channel_name, segment_id, size):
        """Met √† jour les stats de segments pour une cha√Æne"""
        try:
            with self.lock:
                # Init des stats pour cette cha√Æne si n√©cessaire
                if channel_name not in self.stats:
                    logger.warning(f"Attempting to update segment stats for non-existent channel {channel_name}. Skipping.")
                    return

                # Access channel stats directly
                channel_data = self.stats[channel_name]

                # Ensure necessary keys exist
                if "total_segments" not in channel_data: channel_data["total_segments"] = 0
                if "segment_history" not in channel_data: channel_data["segment_history"] = []

                # Mise √† jour du nombre total de segments
                channel_data["total_segments"] += 1

                # Mise √† jour des stats de segments
                segment_history = channel_data.get("segment_history", [])
                segment_history.append(
                    {"segment_id": segment_id, "size": size, "time": int(time.time())}
                )

                # On garde que les 100 derniers segments pour limiter la taille
                channel_data["segment_history"] = segment_history[-100:]

        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour stats segments: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def cleanup_stats(self):
        logger.info("D√©but du nettoyage...")

        # Arr√™t du StatsCollector
        if hasattr(self, "stats_collector"):
            self.stats_collector.stop()
            logger.info("üìä StatsCollector arr√™t√©")

        # Arr√™t du thread d'initialisation
        self.stop_init_thread.set()

        if hasattr(self, "channel_init_thread") and self.channel_init_thread.is_alive():
            self.channel_init_thread.join(timeout=5)

        if hasattr(self, "hls_cleaner"):
            self.hls_cleaner.stop()

        if hasattr(self, "observer"):
            self.observer.stop()
            self.observer.join()

        if hasattr(self, "ready_observer"):
            self.ready_observer.stop()
            self.ready_observer.join()

        for name, channel in self.channels.items():
            channel._clean_processes()

        if hasattr(self, "scan_thread_stop"):
            self.scan_thread_stop.set()

        if hasattr(self, "scan_thread") and self.scan_thread.is_alive():
            self.scan_thread.join(timeout=5)

        logger.info("Nettoyage termin√©")

    def stop(self):
        """Arr√™te proprement le thread de sauvegarde"""
        try:
            logger.info("üõë Arr√™t du StatsCollector...")
            self.stop_save_thread.set()
            if hasattr(self, "save_thread") and self.save_thread.is_alive():
                self.save_thread.join(timeout=5)  # Attendre max 5 secondes
            # Derni√®re sauvegarde avant l'arr√™t
            self.save_stats()
            self.save_user_stats()
            logger.info("‚úÖ StatsCollector arr√™t√© proprement")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'arr√™t du StatsCollector: {e}")

    def _update_daily_stats(self, channel, ip, duration):
        """Met √† jour les statistiques quotidiennes"""
        try:
            # Obtenir la date actuelle
            today = time.strftime("%Y-%m-%d")

            # Initialiser les stats pour aujourd'hui si n√©cessaire
            if today not in self.daily_stats:
                self.daily_stats[today] = {
                    "total_watch_time": 0.0,
                    "unique_viewers": set(),
                    "channels": {}
                }

            # Mise √† jour des stats globales pour aujourd'hui
            daily_stats = self.daily_stats[today]
            daily_stats["total_watch_time"] += duration
            daily_stats["unique_viewers"].add(ip)

            # Initialiser les stats pour ce canal si n√©cessaire
            if channel not in daily_stats["channels"]:
                daily_stats["channels"][channel] = {
                    "total_watch_time": 0.0,
                    "unique_viewers": set()
                }

            # Mise √† jour des stats du canal
            channel_stats = daily_stats["channels"][channel]
            channel_stats["total_watch_time"] += duration
            channel_stats["unique_viewers"].add(ip)

            # Log pour debug
            logger.debug(
                f"üìÖ Stats quotidiennes mises √† jour pour {channel}:\n"
                f"  - Temps ajout√©: {duration:.1f}s\n"
                f"  - Total canal: {channel_stats['total_watch_time']:.1f}s\n"
                f"  - Total journalier: {daily_stats['total_watch_time']:.1f}s"
            )

        except Exception as e:
            logger.error(f"‚ùå Erreur mise √† jour stats quotidiennes: {e}")
            import traceback
            logger.error(traceback.format_exc())

    def handle_segment_request(self, channel, ip, line, user_agent):
        """G√®re une requ√™te de segment et met √† jour les statistiques"""
        with self.lock:
            try:
                # Identifier le segment
                segment_match = re.search(r"segment_(\d+)\.ts", line)
                segment_id = segment_match.group(1) if segment_match else "unknown"

                # Ajouter du temps de visionnage
                segment_duration = 4.0  # secondes par segment
                self.add_watch_time(channel, ip, segment_duration)
                
                # Mettre √† jour les stats utilisateur
                if user_agent:
                    self.update_user_stats(ip, channel, segment_duration, user_agent)

                # Mettre √† jour les stats de segments
                self.update_segment_stats(channel, segment_id, 0)  # Taille inconnue pour l'instant

                logger.debug(f"[{channel}] üìä Segment {segment_id} trait√© pour {ip}")

            except Exception as e:
                logger.error(f"‚ùå Erreur traitement segment: {e}")

    def handle_playlist_request(self, channel, ip, elapsed, user_agent):
        """G√®re une requ√™te de playlist et met √† jour les statistiques"""
        with self.lock:
            try:
                # Ajouter du temps de visionnage
                self.add_watch_time(channel, ip, elapsed)
                
                # Mettre √† jour les stats utilisateur
                if user_agent:
                    self.update_user_stats(ip, channel, elapsed, user_agent)

                logger.debug(f"[{channel}] üìä Playlist trait√©e pour {ip} ({elapsed:.1f}s)")

            except Exception as e:
                logger.error(f"‚ùå Erreur traitement playlist: {e}")

    def handle_channel_change(self, ip, old_channel, new_channel):
        """G√®re un changement de cha√Æne et met √† jour les statistiques"""
        with self.lock:
            try:
                # Mettre √† jour les stats de l'ancienne cha√Æne
                if old_channel:
                    self.update_channel_watchers(old_channel, 0)  # R√©duire le compteur

                # Mettre √† jour les stats de la nouvelle cha√Æne
                if new_channel:
                    self.update_channel_watchers(new_channel, 1)  # Augmenter le compteur

                logger.debug(f"üîÑ Changement de cha√Æne trait√©: {ip} de {old_channel} vers {new_channel}")

            except Exception as e:
                logger.error(f"‚ùå Erreur traitement changement de cha√Æne: {e}")

    def record_buffer_issue(self, channel_name: str, delay: float, viewers: int):
        """Enregistre un probl√®me de buffer pour une cha√Æne"""
        try:
            if channel_name not in self.buffer_issues:
                self.buffer_issues[channel_name] = []
                
            # Ajouter l'incident avec timestamp
            self.buffer_issues[channel_name].append({
                'timestamp': time.time(),
                'delay': delay,
                'viewers': viewers
            })
            
            # Garder seulement les 100 derniers incidents
            if len(self.buffer_issues[channel_name]) > 100:
                self.buffer_issues[channel_name].pop(0)
                
            # Mettre √† jour les m√©triques de performance
            if channel_name not in self.performance_metrics:
                self.performance_metrics[channel_name] = {
                    'buffer_issues_count': 0,
                    'avg_buffer_delay': 0,
                    'max_buffer_delay': 0
                }
                
            metrics = self.performance_metrics[channel_name]
            metrics['buffer_issues_count'] += 1
            metrics['avg_buffer_delay'] = sum(issue['delay'] for issue in self.buffer_issues[channel_name]) / len(self.buffer_issues[channel_name])
            metrics['max_buffer_delay'] = max(issue['delay'] for issue in self.buffer_issues[channel_name])
            
            logger.warning(
                f"[{channel_name}] üìä Probl√®me de buffer enregistr√©: "
                f"d√©lai={delay:.2f}s, viewers={viewers}"
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur enregistrement probl√®me buffer: {e}")
            
    def record_latency_issue(self, channel_name: str, latency: float, viewers: int):
        """Enregistre un probl√®me de latence pour une cha√Æne"""
        try:
            if channel_name not in self.latency_issues:
                self.latency_issues[channel_name] = []
                
            # Ajouter l'incident avec timestamp
            self.latency_issues[channel_name].append({
                'timestamp': time.time(),
                'latency': latency,
                'viewers': viewers
            })
            
            # Garder seulement les 100 derniers incidents
            if len(self.latency_issues[channel_name]) > 100:
                self.latency_issues[channel_name].pop(0)
                
            # Mettre √† jour les m√©triques de performance
            if channel_name not in self.performance_metrics:
                self.performance_metrics[channel_name] = {
                    'latency_issues_count': 0,
                    'avg_latency': 0,
                    'max_latency': 0
                }
                
            metrics = self.performance_metrics[channel_name]
            metrics['latency_issues_count'] += 1
            metrics['avg_latency'] = sum(issue['latency'] for issue in self.latency_issues[channel_name]) / len(self.latency_issues[channel_name])
            metrics['max_latency'] = max(issue['latency'] for issue in self.latency_issues[channel_name])
            
            logger.warning(
                f"[{channel_name}] üìä Probl√®me de latence enregistr√©: "
                f"latence={latency:.2f}s, viewers={viewers}"
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erreur enregistrement probl√®me latence: {e}")
            
    def get_performance_metrics(self, channel_name: str = None):
        """R√©cup√®re les m√©triques de performance pour une cha√Æne ou toutes les cha√Ænes"""
        try:
            if channel_name:
                return self.performance_metrics.get(channel_name, {})
            return self.performance_metrics
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration m√©triques performance: {e}")
            return {}
